{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7662f793",
   "metadata": {},
   "source": [
    "#                               IMDB Reviews Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fe337",
   "metadata": {},
   "source": [
    "**Dilruba Benzerler 090190309 benzerler19@itu.edu.tr**\n",
    "\n",
    "**Kaan Kaymaz 090180333 kaymaz18@itu.edu.tr**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81f2b8",
   "metadata": {},
   "source": [
    "### Dataset Link and Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c4c01",
   "metadata": {},
   "source": [
    "This [dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/code) involves reviews of some random films imported from IMBD. It has two columns which are \"text\" and \"label. \"text\" contains text of reviews of films and \"label\" contains the sentiment of the reviews as \"0\",\"1\". The dateset includes 20000 rows of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60437bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2497c824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./imdb-dataset-sentiment-analysis-in-csv-format\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format/download?datasetVersionNumber=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d12e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b34754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data, columns = ['sentiment'])\n",
    "df.drop(\"sentiment_negative\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad3736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"review\":\"text\",\"sentiment_positive\":\"label\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f502f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fb680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaankaymaz/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df[\"text\"]=[BeautifulSoup(i).get_text() for i in df['text']]\n",
    "# To clean html tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f268f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  One of the other reviewers has mentioned that ...      1\n",
       "1  A wonderful little production. The filming tec...      1\n",
       "2  I thought this was a wonderful way to spend ti...      1\n",
       "3  Basically there's a family where a little boy ...      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2da0dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1d226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    20000 non-null  object\n",
      " 1   label   20000 non-null  uint8 \n",
      "dtypes: object(1), uint8(1)\n",
      "memory usage: 175.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25042b5c",
   "metadata": {},
   "source": [
    "**Our data consists of 20000 lines and 2 columns, consisting of a \"text\" column with the reviews of movies and a \"label\" column containing the labels indicating positivity or negativity according to the emotion of the review.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52644888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330cdd9",
   "metadata": {},
   "source": [
    "**Text column contains some reviews about random films on IMDB.com.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadb9d8",
   "metadata": {},
   "source": [
    "**Data is well organized and it has a neat format. Therefore, it can be said that the data is structured. The only regulation that has to be done is to convert text data to numeric data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fde88",
   "metadata": {},
   "source": [
    "***In order to make our data in text form usable, some Natural Language Proccesing operations will be performed. NLP operations contains some proccesses which will perform some cleaning on text. There are special charachters in texts.A special character is a character that is not an alphabetic or numeric character. Punctuation marks and other symbols are examples of special characters. To get rid of these we will use some functions which is in \"re\" library. Next step is tokenization. Tokenization splits sentences into individual words. Then, we will remove stopwords like \"a\",\"and\", \"the\". Other step is lemmatization which is the grouping together of different forms of the same word. Last step is vectorization which converts text data to numeric data. With this step, we made our text data, which was unusable, usable by making it numerical. These functions that we will be using are built in functions in \"nltk\" library.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da288e12",
   "metadata": {},
   "source": [
    "**Below is the final numerized version of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a1b85f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaahhhhhhggg</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaargh</th>\n",
       "      <th>aaaaarrrrrrgggggghhhhhh</th>\n",
       "      <th>aaaaaw</th>\n",
       "      <th>aaaahhhhhhh</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaaggghhhhhhh</th>\n",
       "      <th>...</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zy</th>\n",
       "      <th>zzzz</th>\n",
       "      <th>zzzzip</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 61102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaa  aaaaaaahhhhhhggg  aaaaah  aaaaargh  aaaaarrrrrrgggggghhhhhh  \\\n",
       "0       0    0                 0       0         0                        0   \n",
       "1       0    0                 0       0         0                        0   \n",
       "2       0    0                 0       0         0                        0   \n",
       "3       0    0                 0       0         0                        0   \n",
       "4       0    0                 0       0         0                        0   \n",
       "...    ..  ...               ...     ...       ...                      ...   \n",
       "19995   0    0                 0       0         0                        0   \n",
       "19996   0    0                 0       0         0                        0   \n",
       "19997   0    0                 0       0         0                        0   \n",
       "19998   0    0                 0       0         0                        0   \n",
       "19999   0    0                 0       0         0                        0   \n",
       "\n",
       "       aaaaaw  aaaahhhhhhh  aaaand  aaaggghhhhhhh  ...  zwick  zy  zzzz  \\\n",
       "0           0            0       0              0  ...      0   0     0   \n",
       "1           0            0       0              0  ...      0   0     0   \n",
       "2           0            0       0              0  ...      0   0     0   \n",
       "3           0            0       0              0  ...      0   0     0   \n",
       "4           0            0       0              0  ...      0   0     0   \n",
       "...       ...          ...     ...            ...  ...    ...  ..   ...   \n",
       "19995       0            0       0              0  ...      0   0     0   \n",
       "19996       0            0       0              0  ...      0   0     0   \n",
       "19997       0            0       0              0  ...      0   0     0   \n",
       "19998       0            0       0              0  ...      0   0     0   \n",
       "19999       0            0       0              0  ...      0   0     0   \n",
       "\n",
       "       zzzzip  zzzzzzzzz  zzzzzzzzzzzz  zzzzzzzzzzzzz  \\\n",
       "0           0          0             0              0   \n",
       "1           0          0             0              0   \n",
       "2           0          0             0              0   \n",
       "3           0          0             0              0   \n",
       "4           0          0             0              0   \n",
       "...       ...        ...           ...            ...   \n",
       "19995       0          0             0              0   \n",
       "19996       0          0             0              0   \n",
       "19997       0          0             0              0   \n",
       "19998       0          0             0              0   \n",
       "19999       0          0             0              0   \n",
       "\n",
       "       zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                    0   \n",
       "1                                    0   \n",
       "2                                    0   \n",
       "3                                    0   \n",
       "4                                    0   \n",
       "...                                ...   \n",
       "19995                                0   \n",
       "19996                                0   \n",
       "19997                                0   \n",
       "19998                                0   \n",
       "19999                                0   \n",
       "\n",
       "       zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              0   \n",
       "3                                              0   \n",
       "4                                              0   \n",
       "...                                          ...   \n",
       "19995                                          0   \n",
       "19996                                          0   \n",
       "19997                                          0   \n",
       "19998                                          0   \n",
       "19999                                          0   \n",
       "\n",
       "       zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                                 0  \n",
       "1                                                 0  \n",
       "2                                                 0  \n",
       "3                                                 0  \n",
       "4                                                 0  \n",
       "...                                             ...  \n",
       "19995                                             0  \n",
       "19996                                             0  \n",
       "19997                                             0  \n",
       "19998                                             0  \n",
       "19999                                             0  \n",
       "\n",
       "[20000 rows x 61102 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5b277",
   "metadata": {},
   "source": [
    "**text_vectorized dataframe will be taken as independent and df[\"label\"] column will be taken as dependent variable while working the model. Algorithms specified in the Method and Algorithms section will be applied and accuracy will be calculated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d847af",
   "metadata": {},
   "source": [
    "### Object of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b92f3",
   "metadata": {},
   "source": [
    "Since the text data is a data type that is difficult to work with, we should make it numeric, which is easier to work with. Hence,\n",
    "\n",
    "   *1. The first aim is to converting text data to numeric data.*\n",
    "\n",
    "The label column of the dataset is a column formed by outputting 0 if the comment is negative and 1 if it is positive, depending on what type of emotional expressions the movie comments contain, positive or negative. Therefore,\n",
    "\n",
    "   *2. Second aim is to check the accuracy of this column.*\n",
    "\n",
    "When the data was examined, it was understood that the reviews could be grouped under certain topics. Therefore,\n",
    "\n",
    "   *3. Our third aim is to identify the topics that are mainly mentioned in the reviews and to classify the reviews under these topics.*\n",
    "   \n",
    "   *4. Fourth aim can be explained as follows: apart from the label column given in the data, new label column will be created using sentiment analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf790a7",
   "metadata": {},
   "source": [
    "### Methods and Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d2610",
   "metadata": {},
   "source": [
    "To test the first hypothesis we will use \"nltk\", \"re\" and \"spacy\" libraries. Using these libraries, we separate sentences into words by getting rid of stopwords and specials. And in the last step, we make this data numerical using words.\n",
    "\n",
    "To test our second hypothesis Logistic Regression, Naive Bayes, Decision Tree and Random Forest algorithms will be used which will give the result whether the \"label\" column is correct or not. And interpretation will be made according to the accuracy value to be obtained from this model. \n",
    "\n",
    "In order to test our third hypothesis, we need to create the clusters. K-Means algorithm, LDA(Latent Dirichlet Allocation) will be used to generate clusters.\n",
    "\n",
    "To test fourth hypothesis there will be created an Pipeline to clean, tokenize, vectorize, and classify using \"Count Vectorizor\". SVM algorithm will be run using Pipeline and new labels will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf301caa",
   "metadata": {},
   "source": [
    "### Task Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e30703",
   "metadata": {},
   "source": [
    "Basically the whole project will be carried out together. However, Dilruba will work predominantly at the first and second hypothesis, while Kaan will predominantly work in the third and fourth hypothesis. So Dilruba will take part in the search for \"nltk\" and \"spacy\" libraries. Kaan will explore how to do topic modeling with \"LDA\". In addition, which algorithms will be used in the project and how the algorithms to be used should be discussed and decided together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28b201",
   "metadata": {},
   "source": [
    "### Calendar\n",
    "| **Task**                                                  | **Deadline**| \n",
    "|:---------------------|:------------------|\n",
    "|   Finding and examining the dataset                       |   Nov 14  |\n",
    "|   Planning the whole project and completing the proposal  |   Dec 2     |\n",
    "|   Executing first two aims of the project                               |   Dec 9     | \n",
    "|   Executing last two aims of the project                                |   Dec 16    | \n",
    "|   Checking/comparing results                              |   Dec 23    | \n",
    "|   Project Delivery                                        |   Dec 30    | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b2f7b",
   "metadata": {},
   "source": [
    "There may be changes or additions about deadlines and tasks according to the progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a00b7",
   "metadata": {},
   "source": [
    "# Atabey's notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca8e74",
   "metadata": {},
   "source": [
    "You must provide some samples of the dataset(s) and explain what they contain. What is the shape of the dataset? What features does it have? Is the data structured or unstructured? How are you planning on converting text into a usable dataset?\n",
    "\n",
    "Your questions are too vague. How are you planning on testing the accuracy of sentiments attached to each review? How are you going to figure out topics? What do you mean by 'sentiment' and what do you mean by 'topic'? How are you going to model them? What type of ML models are there that would help you answer this question?\n",
    "\n",
    "This proposal is weak. It reads like something written in less than an hour. Also, the data is fairly well studied: there are close to 50 notebooks that studied this data on kaggle only. You must do something substantially different from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070c2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "729400435fcd1b32da66da2ed38a13815f14104876c12b88e9c1e12c42c97318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
