{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de212311",
   "metadata": {},
   "source": [
    "                                                                            Dilruba Benzerler 090190309\n",
    "                                                                            Kaan Kaymaz 090180333"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd47d0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in IMDB Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1057fde",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd32b90",
   "metadata": {},
   "source": [
    "In the subject selection of the project, it was decided to use sentiment analysis models based on the idea of examining the IMBD Review data.\n",
    "\n",
    "Since it was decided that it was insufficient to decide that the reviews were only positive or negative, it was thought that the use of topic modeling could enrich the project.\n",
    "\n",
    "After the subjects were decided, it was decided that some libraries such as (SVM, LDA, NLTK, KMeans, Sentiment Intensity Analyzer) would be appropriate to use during the research phase.\n",
    "\n",
    "New label and topic classes were created using models and libraries. And analyzed by looking at their scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358df7a3",
   "metadata": {},
   "source": [
    "## Metodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec24fe49",
   "metadata": {},
   "source": [
    "Some library, model, and function names and functions used during the project are given below:\n",
    "\n",
    "    1) BeautifulSoup: Beautiful Soup is a Python library that is used for web scraping purposes. It is designed to make it easy to navigate, search, and modify the parse tree of an HTML or XML document. With Beautiful Soup, you can parse an HTML or XML document and extract the data that you need.\n",
    "    \n",
    "    2) SentimentIntensityAnalyzer: SentimentIntensityAnalyzer is a class within the vaderSentiment library in Python that is used for performing sentiment analysis. Sentiment analysis is the process of determining the sentiment or emotion behind a given piece of text. The \"SentimentIntensityAnalyzer\" class provides a convenient interface for analyzing the sentiment of a given piece of text and generating a score for each of four sentiment categories: positive, negative, neutral, and compound.\n",
    "    \n",
    "    3) NLTK: The Natural Language Toolkit (NLTK) is a Python library for working with human language data (text). It provides a wide range of tools for tasks such as tokenization (splitting text into individual words and punctuation), part-of-speech tagging (labeling words as nouns, verbs, etc.), and parsing (analyzing the structure of a sentence). NLTK also includes a large collection of text data (corpora) and lexical resources (such as word lists and wordnet) that can be used for natural language processing tasks.\n",
    "    \n",
    "    4) LDA: Latent Dirichlet Allocation (LDA) is a technique for identifying the underlying themes or topics in a collection of documents. It is a probabilistic model that represents each document as a mixture of a fixed number of topics and each word as generated from one of the document's topics. LDA is used for tasks such as information retrieval, document classification, and topic summarization. It can also be used to understand the latent structure of a collection of documents and discover the main themes or topics in the documents.\n",
    "    \n",
    "    5) SVM: Support Vector Machines (SVMs) are a type of machine learning algorithm that can be used to classify data. They find the best line or plane to separate different classes in the data. The line or plane is chosen so that it is as far as possible from all the data points. The points closest to the line or plane are called \"support vectors.\" SVMs are often used for classification tasks and can handle high-dimensional data well, but they can be sensitive to the way the data is split and can be slow to train on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb862a",
   "metadata": {},
   "source": [
    "### Base Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b4e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import opendatasets as od\n",
    "import sklearn.metrics as metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f5cad",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b71735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: kaankaymaz\n",
      "Your Kaggle Key: ········\n",
      "Downloading imdb-dataset-of-50k-movie-reviews.zip to ./imdb-dataset-of-50k-movie-reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 25.7M/25.7M [00:02<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/download?datasetVersionNumber=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dccda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7172997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data, columns = ['sentiment'])\n",
    "df.drop(\"sentiment_negative\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32c0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"review\":\"text\",\"sentiment_positive\":\"label\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ab2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5defb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]=[BeautifulSoup(i).get_text() for i in df['text']]\n",
    "# To clean html tags there were called BeautifulSoup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d128d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d99454d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.182, 'neu': 0.752, 'pos': 0.066, 'compound': -0.9916}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6d4ef",
   "metadata": {},
   "source": [
    "**SentimentIntensityAnalyzer returns 'compound' score and this score gives is used to decide whether it is positive or negative. If the compound is negative, the sentence is labeled negative, if it is positive, it is labeled positive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f445838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(sentiment_dict):\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return(\"Positive\")\n",
    "    else :\n",
    "        return(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412c1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label = []\n",
    "for i in range(len(df)):\n",
    "    sentiment_dict = sentiment.polarity_scores(df[\"text\"][i])\n",
    "    list_label.append(labeler(sentiment_dict))\n",
    "df[\"label_analyzer\"] = list_label\n",
    "df[\"label_analyzer\"].replace(\"Negative\",\"0\",inplace=True)\n",
    "df[\"label_analyzer\"].replace(\"Positive\",\"1\",inplace=True)\n",
    "df[\"label_analyzer\"] = df[\"label_analyzer\"].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa25588",
   "metadata": {},
   "source": [
    "**SentimentIntensityAnalyzer is a pre-trained machine learning model that is part of the Natural Language Toolkit (NLTK), a collection of libraries and resources for natural language processing (NLP) in Python.**\n",
    "\n",
    "**The SentimentIntensityAnalyzer is used to analyze the sentiment (positive, negative, or neutral) of a piece of text. It does this by assigning a score to each piece of text based on the words and phrases it contains, as well as various other factors such as punctuation and capitalization. The score is a numerical value that ranges from -1 (strongly negative) to 1 (strongly positive), with 0 indicating neutral sentiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac40471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity ratio between data's own label and label analyzer: 0.723\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity ratio between data's own label and label analyzer:\",(df[\"label\"] == df[\"label_analyzer\"]).sum()/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de4219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_text(text):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.cluster import KMeans\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(2,10)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
    "        km = km.fit(X)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    print('How many clusters do you want to use?')\n",
    "    true_k = int(input())\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
    "    model.fit(X)\n",
    "\n",
    "    labels=model.labels_\n",
    "    clusters=pd.DataFrame(list(zip(text,labels)),columns=['title','cluster'])\n",
    "    df[\"label_kmeans\"]=clusters[\"cluster\"]\n",
    "    \n",
    "    for i in range(true_k):\n",
    "        print(clusters[clusters['cluster'] == i])\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d5c42",
   "metadata": {},
   "source": [
    "**Using the KMeans algorithm, the texts are classified according to their topic and assigned to the \"label_kmeans\" column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd9f7f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many clusters do you want to use?\n",
      "7\n",
      "                                                 title  cluster\n",
      "44   This movie struck home for me. Being 29, I rem...        0\n",
      "259  Back in 1994, I had a really lengthy vacation ...        0\n",
      "534  I just got this video used and I was watching ...        0\n",
      "660  I was never so bored in my life. Hours of pret...        0\n",
      "929  I saw this movie in a theater while on vacatio...        0\n",
      "                                                 title  cluster\n",
      "6    I sure would like to see a resurrection of a u...        1\n",
      "9    If you like original gut wrenching laughter yo...        1\n",
      "22   What an absolutely stunning movie, if you have...        1\n",
      "62   So let's begin!)))The movie itself is as origi...        1\n",
      "97   Well, I like to watch bad horror B-Movies, cau...        1\n",
      "..                                                 ...      ...\n",
      "960  This was my first Gaspar Noe movie I've watche...        1\n",
      "965  I never want to see this movie again!Not only ...        1\n",
      "980  Now I don't hate cheap movies. I just don't se...        1\n",
      "992  Big Fat Liar is what you get when you combine ...        1\n",
      "998  If you like me is going to see this in a film ...        1\n",
      "\n",
      "[109 rows x 2 columns]\n",
      "                                                 title  cluster\n",
      "2    I thought this was a wonderful way to spend ti...        2\n",
      "5    Probably my all-time favorite movie, a story o...        2\n",
      "20   After the success of Die Hard and it's sequels...        2\n",
      "25   The Karen Carpenter Story shows a little more ...        2\n",
      "39   After sitting through this pile of dung, my hu...        2\n",
      "..                                                 ...      ...\n",
      "951  Had it with the one who raised you since when ...        2\n",
      "967  Life Begins - and ends - in a typical 1930's m...        2\n",
      "974  Whatever rating I give BOOM is only because of...        2\n",
      "983  For a film made in Senegal, based, I guess loo...        2\n",
      "994  On watching this film, I was amazed at how med...        2\n",
      "\n",
      "[85 rows x 2 columns]\n",
      "                                                 title  cluster\n",
      "17   This movie made it into one of my top 10 most ...        3\n",
      "21   I had the terrible misfortune of having to vie...        3\n",
      "23   First of all, let's get a few things straight ...        3\n",
      "24   This was the worst movie I saw at WorldFest an...        3\n",
      "26   \"The Cell\" is an exotic masterpiece, a dizzyin...        3\n",
      "..                                                 ...      ...\n",
      "976  I had never heard of Larry Fessenden before bu...        3\n",
      "977  Schlocky '70s horror films...ya gotta love 'em...        3\n",
      "978  I suppose for 1961 this film was supposed to b...        3\n",
      "989  This movie was way over-hyped. A lot of the vi...        3\n",
      "997  I usually try to be professional and construct...        3\n",
      "\n",
      "[190 rows x 2 columns]\n",
      "                                                 title  cluster\n",
      "0    One of the other reviewers has mentioned that ...        4\n",
      "7    This show was an amazing, fresh & innovative i...        4\n",
      "11   I saw this movie when I was about 12 when it c...        4\n",
      "12   So im not a big fan of Boll's work but then ag...        4\n",
      "15   Kind of drawn in by the erotic scenes, only to...        4\n",
      "..                                                 ...      ...\n",
      "970  *Criticism does mention spoilers*I rarely make...        4\n",
      "985  The Royal Rumble has traditionally been one of...        4\n",
      "986  I remember this in a similar vein to the Young...        4\n",
      "995  Nothing is sacred. Just ask Ernie Fosselius. T...        4\n",
      "999  This is like a zoology textbook, given that it...        4\n",
      "\n",
      "[226 rows x 2 columns]\n",
      "                                                 title  cluster\n",
      "3    Basically there's a family where a little boy ...        5\n",
      "13   The cast played Shakespeare.Shakespeare lost.I...        5\n",
      "14   This a fantastic movie of three prisoners who ...        5\n",
      "46   Protocol is an implausible movie whose only sa...        5\n",
      "47   How this film could be classified as Drama, I ...        5\n",
      "..                                                 ...      ...\n",
      "975  VIVA LA BAM This \"Jackass\" spin off focuses on...        5\n",
      "982  As soon as it hits a screen, it destroys all i...        5\n",
      "987  Destined to be a classic before it was even co...        5\n",
      "993  Tim Robbins and John Cusack are two actors I h...        5\n",
      "996  I hated it. I hate self-aware pretentious inan...        5\n",
      "\n",
      "[125 rows x 2 columns]\n",
      "                                                 title  cluster\n",
      "1    A wonderful little production. The filming tec...        6\n",
      "4    Petter Mattei's \"Love in the Time of Money\" is...        6\n",
      "8    Encouraged by the positive comments about this...        6\n",
      "10   Phil the Alien is one of those quirky films wh...        6\n",
      "16   Some films just simply should not be remade. T...        6\n",
      "..                                                 ...      ...\n",
      "981  A fabulous book about a fox and his family who...        6\n",
      "984  This is a very noir kind of episode. It begins...        6\n",
      "988  Stephane Audran is the eponymous heroine of th...        6\n",
      "990  It is incredible that there were two films wit...        6\n",
      "991  I dunno sometimes...you try and try and try to...        6\n",
      "\n",
      "[260 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cluster_text(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039d5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1219dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run the code\n",
    "def rid_of_specials(words):\n",
    "    new= ''\n",
    "    for i in range(len(words)):\n",
    "        a = re.sub('[^A-Za-z]+', ' ', words[i]).lower()\n",
    "        new += a\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f72b583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [rid_of_specials(i) for i in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70771af",
   "metadata": {},
   "source": [
    "**The texts of the data contain some numbers, punctuations, and capitalized words that make it difficult to work with the data. These need to be gotten rid of. There is written a function to get rid of them. The function takes the texts as input and uses the \"sub\" function of the \"Regular Expression\" library to bring the words or expressions we want to the unwanted places in the texts. Since we wanted to get rid of punctuation marks, we entered the \"[^A-Za-z]\" command inside the function, which removes punctuations. And we filled them with space. At the end, we used the \".lower()\" function because we wanted to make the sentence lowercase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81b38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=[word_tokenize(i) for i in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50188f",
   "metadata": {},
   "source": [
    "**Here the tokenization process is done. Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83793471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#Step 2\n",
    "stopped = [[i for i in j if i not in stop_words] for j in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7a8f7",
   "metadata": {},
   "source": [
    "**A stop word is a commonly used word that a search engine is programmed to ignore both when indexing search entries and when retrieving them as a result. Words like [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"] are examples. These words are not wanted to take up space in our database or take up valuable processing time. So there is ran a function to remove those words. English stop words in \"Step 1\" are given by the list. In \"Step2\", there were created a nested for loop to search for these words among the words we have separated above as \"tokenized\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fc79a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [[lemmatizer.lemmatize(i) for i in j] for j in stopped]\n",
    "\n",
    "prepeared_sentence= [' '.join(j) for j in lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a446195",
   "metadata": {},
   "source": [
    "**Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. Examples: rocks -> rock, better -> good, feet -> foot, (am, are, is) -> be.\n",
    "That is why there is called WordNetLemmatizer function and texts were lemmatized.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8592a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(prepeared_sentence)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "text_vectorized = pd.DataFrame(X.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19858d",
   "metadata": {},
   "source": [
    "**Finally, the CountVectorizer function was called to make the lemmatized words numerical and our text data was made numerical. This numericized data is assigned to a dataframe named text_vectorized.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e674f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbot</th>\n",
       "      <th>...</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoology</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zp</th>\n",
       "      <th>zu</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 15763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aaargh  aaliyah  aamir  aaron  ab  abandon  abandoned  abba  abbey  \\\n",
       "0         0        0      0      0   0        0          0     0      0   \n",
       "1         0        0      0      0   0        0          0     0      0   \n",
       "2         0        0      0      0   0        0          0     0      0   \n",
       "3         0        0      0      0   0        0          0     0      0   \n",
       "4         0        0      0      0   0        0          0     0      0   \n",
       "..      ...      ...    ...    ...  ..      ...        ...   ...    ...   \n",
       "995       0        0      0      0   0        0          0     0      0   \n",
       "996       0        0      0      0   0        0          0     0      0   \n",
       "997       0        0      0      0   0        0          0     0      0   \n",
       "998       0        0      0      0   0        0          0     0      0   \n",
       "999       0        0      0      0   0        0          0     0      0   \n",
       "\n",
       "     abbot  ...  zoo  zoology  zoom  zooming  zp  zu  zucker  zulu  zwick  \\\n",
       "0        0  ...    0        0     0        0   0   0       0     0      0   \n",
       "1        0  ...    0        0     0        0   0   0       0     0      0   \n",
       "2        0  ...    0        0     0        0   0   0       0     0      0   \n",
       "3        0  ...    0        0     0        0   0   0       0     0      0   \n",
       "4        0  ...    0        0     0        0   0   0       0     0      0   \n",
       "..     ...  ...  ...      ...   ...      ...  ..  ..     ...   ...    ...   \n",
       "995      0  ...    0        0     0        0   0   0       0     0      0   \n",
       "996      0  ...    0        0     0        0   0   0       0     0      0   \n",
       "997      0  ...    0        0     0        0   0   0       0     0      0   \n",
       "998      0  ...    0        0     0        0   0   0       0     0      0   \n",
       "999      0  ...    0        1     0        0   0   0       0     0      0   \n",
       "\n",
       "     zzzzzzzzzzzzzzzzzz  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "..                  ...  \n",
       "995                   0  \n",
       "996                   0  \n",
       "997                   0  \n",
       "998                   0  \n",
       "999                   0  \n",
       "\n",
       "[1000 rows x 15763 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75ebe18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 3), (23, 1), (24, 2), (25, 1), (26, 2), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 4), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 4), (68, 1), (69, 2), (70, 1), (71, 2), (72, 1)]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(lemmatized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in lemmatized]\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc9473",
   "metadata": {},
   "source": [
    "**Using the above processes, it was found that the number of unique words formed as a result of the lemmatization process was 73. And these words were divided into 73 separate classes for use in the model and assigned to the \"corpus\" variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983cf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 7, id2word=dictionary, passes=15)\n",
    "ldamodel.save('mOdel.gensim')\n",
    "topics = ldamodel.print_topics(num_words=30) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7760c46",
   "metadata": {},
   "source": [
    "**Here, too, it can be seen what the unique 73 words are.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a199f425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('appeal', 1),\n",
       "  ('around', 1),\n",
       "  ('audience', 1),\n",
       "  ('away', 2),\n",
       "  ('become', 1),\n",
       "  ('called', 1),\n",
       "  ('city', 2),\n",
       "  ('class', 1),\n",
       "  ('classic', 1),\n",
       "  ('death', 1),\n",
       "  ('developed', 1),\n",
       "  ('drug', 1),\n",
       "  ('due', 2),\n",
       "  ('episode', 2),\n",
       "  ('ever', 1),\n",
       "  ('exactly', 1),\n",
       "  ('experience', 1),\n",
       "  ('face', 1),\n",
       "  ('fact', 1),\n",
       "  ('far', 1),\n",
       "  ('first', 2),\n",
       "  ('focus', 1),\n",
       "  ('forget', 3),\n",
       "  ('front', 1),\n",
       "  ('get', 2),\n",
       "  ('given', 1),\n",
       "  ('go', 2),\n",
       "  ('got', 1),\n",
       "  ('happened', 1),\n",
       "  ('high', 2),\n",
       "  ('home', 1),\n",
       "  ('italian', 1),\n",
       "  ('kill', 1),\n",
       "  ('lack', 1),\n",
       "  ('level', 1),\n",
       "  ('main', 1),\n",
       "  ('many', 1),\n",
       "  ('may', 1),\n",
       "  ('mentioned', 1),\n",
       "  ('mess', 1),\n",
       "  ('middle', 1),\n",
       "  ('nasty', 1),\n",
       "  ('never', 1),\n",
       "  ('order', 1),\n",
       "  ('picture', 1),\n",
       "  ('pretty', 1),\n",
       "  ('pull', 1),\n",
       "  ('ready', 1),\n",
       "  ('regard', 1),\n",
       "  ('reviewer', 1),\n",
       "  ('right', 2),\n",
       "  ('romance', 1),\n",
       "  ('saw', 1),\n",
       "  ('say', 2),\n",
       "  ('scene', 1),\n",
       "  ('set', 1),\n",
       "  ('sex', 1),\n",
       "  ('show', 4),\n",
       "  ('side', 1),\n",
       "  ('state', 1),\n",
       "  ('street', 1),\n",
       "  ('taste', 1),\n",
       "  ('thing', 1),\n",
       "  ('touch', 1),\n",
       "  ('turned', 1),\n",
       "  ('use', 1),\n",
       "  ('viewing', 1),\n",
       "  ('violence', 4),\n",
       "  ('watched', 1),\n",
       "  ('watching', 2),\n",
       "  ('well', 1),\n",
       "  ('word', 2),\n",
       "  ('would', 1)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6798b889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4793</td>\n",
       "      <td>show, character, would, episode, time, good, g...</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>life, scene, well, character, time, also, stor...</td>\n",
       "      <td>a wonderful little production  the filming tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>like, really, good, bad, funny, get, see, thin...</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>like, best, game, good, get, would, make, life...</td>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.7856</td>\n",
       "      <td>life, scene, well, character, time, also, stor...</td>\n",
       "      <td>petter mattei s  love in the time of money  is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>see, story, world, time, like, character, woul...</td>\n",
       "      <td>nothing is sacred  just ask ernie fosselius  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>see, story, world, time, like, character, woul...</td>\n",
       "      <td>i hated it  i hate self aware pretentious inan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8978</td>\n",
       "      <td>like, even, make, horror, get, bad, thing, wou...</td>\n",
       "      <td>i usually try to be professional and construct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>like, really, good, bad, funny, get, see, thin...</td>\n",
       "      <td>if you like me is going to see this in a film ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>like, really, good, bad, funny, get, see, thin...</td>\n",
       "      <td>this is like a zoology textbook  given that it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic  Perc_Contribution  \\\n",
       "0                 0             0.4793   \n",
       "1                 3             0.9828   \n",
       "2                 5             0.9834   \n",
       "3                 6             0.9785   \n",
       "4                 3             0.7856   \n",
       "..              ...                ...   \n",
       "995               2             0.9865   \n",
       "996               2             0.8701   \n",
       "997               1             0.8978   \n",
       "998               5             0.9678   \n",
       "999               5             0.4967   \n",
       "\n",
       "                                        Topic_Keywords  \\\n",
       "0    show, character, would, episode, time, good, g...   \n",
       "1    life, scene, well, character, time, also, stor...   \n",
       "2    like, really, good, bad, funny, get, see, thin...   \n",
       "3    like, best, game, good, get, would, make, life...   \n",
       "4    life, scene, well, character, time, also, stor...   \n",
       "..                                                 ...   \n",
       "995  see, story, world, time, like, character, woul...   \n",
       "996  see, story, world, time, like, character, woul...   \n",
       "997  like, even, make, horror, get, bad, thing, wou...   \n",
       "998  like, really, good, bad, funny, get, see, thin...   \n",
       "999  like, really, good, bad, funny, get, see, thin...   \n",
       "\n",
       "                                                  text  \n",
       "0    one of the other reviewers has mentioned that ...  \n",
       "1    a wonderful little production  the filming tec...  \n",
       "2    i thought this was a wonderful way to spend ti...  \n",
       "3    basically there s a family where a little boy ...  \n",
       "4    petter mattei s  love in the time of money  is...  \n",
       "..                                                 ...  \n",
       "995  nothing is sacred  just ask ernie fosselius  t...  \n",
       "996  i hated it  i hate self aware pretentious inan...  \n",
       "997  i usually try to be professional and construct...  \n",
       "998  if you like me is going to see this in a film ...  \n",
       "999  this is like a zoology textbook  given that it...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dominant_topic(ldamodel,corpus,content):\n",
    "#Function to find the dominant topic in each query\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each query\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "# Get the Dominant topic, Perc Contribution and Keywords for each query\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # =&gt; dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num,topn=30)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(content)#noisy data\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_dominant_topic = dominant_topic(ldamodel=ldamodel, corpus=corpus, content=df[\"text\"])\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "491c2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_lda\"] = df_dominant_topic[\"Dominant_Topic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eddf3f",
   "metadata": {},
   "source": [
    "**A new label column \"label_lda\" has been created using the above model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1287841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df = 0.9, min_df=2, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42cc43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe87aa",
   "metadata": {},
   "source": [
    "**The LDA model was used here as well, but the LDA model in the above model is used as \"LdaModel\" from the gensim.models library, while here it is used as \"LatentDirichletAllocation\" from the scikit learn library. And they both create different labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f0cae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aee56ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf441f8",
   "metadata": {},
   "source": [
    "**Below, it can be seen in more detail according to which words the topics are separated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36a6fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['really', 'does', 'scenes', 'think', 'people', 'good', 'man', 'way', 'films', 'just', 'time', 'story', 'like', 'movie', 'film']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['woman', 'man', 'way', 'old', 'life', 'love', 'story', 'great', 'good', 'really', 'just', 'best', 'movie', 'like', 'film']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['new', 'young', 'way', 'does', 'really', 'characters', 'time', 'good', 'people', 'like', 'story', 'life', 'just', 'movie', 'film']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['great', 'people', 'watch', 'story', 'don', 'really', 'time', 'think', 'movies', 'bad', 'good', 'film', 'like', 'just', 'movie']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['love', 'make', 'performance', 'story', 'character', 'good', 'life', 'really', 'movie', 'great', 'just', 'way', 'time', 'like', 'film']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['wilson', 'time', 'great', 'just', 'production', 'trek', 'flynn', 'visconti', 'john', 'little', 'jane', 'seen', 'scene', 'charlie', 'like']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['did', 'way', 'films', 'watch', 'make', 'story', 'plot', 'time', 'people', 'good', 'really', 'like', 'just', 'movie', 'film']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "648fff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1e9416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_lda_2\"] = topic.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a20fe70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fbd21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "219e07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02ce184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_countvect = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', cv),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "pipe_countvect.fit(X_train,y_train)\n",
    "\n",
    "sample_prediction = pipe_countvect.predict(X_test)\n",
    "\n",
    "liste1 = []\n",
    "liste2 = []\n",
    "#for (sample,pred) in zip(X_test,sample_prediction):\n",
    "    #print(sample,\"Prediction=>\",pred)       \n",
    "        #When these comment lines are executed, the label is displayed opposite each text line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a18252",
   "metadata": {},
   "source": [
    "**Using the Pipeline, the text input is first cleared with \"predictors\". Data formed after vectorizing with CountVectorizer.Then, with using Support Vector Machine algorithm classes are created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53a04e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.844\n",
      "Accuracy:  1.0\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",pipe_countvect.score(X_test,y_test))\n",
    "print(\"Accuracy: \",pipe_countvect.score(X_test,sample_prediction))\n",
    "# Accuracy\n",
    "print(\"Accuracy: \",pipe_countvect.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c4f9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_pipe = []\n",
    "liste_str = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a7906d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    liste_pipe.append(pipe_countvect.predict([df[\"text\"][i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e140349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    liste_str.append(np.array2string(liste_pipe[i], precision=2, separator=',',\n",
    "                      suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b5391b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_pipeline\"]=liste_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0afb2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_pipeline\"]=df[\"label_pipeline\"].str.replace(\"[\",\"\")\n",
    "df[\"label_pipeline\"]=df[\"label_pipeline\"].str.replace(\"]\",\"\")\n",
    "df[\"label_pipeline\"]=df[\"label_pipeline\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "167c5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity ratio between data's own label and label_pipeline: 0.961\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity ratio between data's own label and label_pipeline:\",(df[\"label\"] == df[\"label_pipeline\"]).sum()/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74d352",
   "metadata": {},
   "source": [
    "**Classes created with Pipeline are assigned to a column called \"label_pipeline\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe29a675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_countvect.predict([\"You are bad at this game.\"]) \n",
    "#By trying examples here, there can better understand how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d056a6f",
   "metadata": {},
   "source": [
    "**As can be seen below, we finally have 3 binary classes and 3 multiclass classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b63a27a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_analyzer</th>\n",
       "      <th>label_kmeans</th>\n",
       "      <th>label_lda</th>\n",
       "      <th>label_lda_2</th>\n",
       "      <th>label_pipeline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production  the filming tec...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s  love in the time of money  is...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>nothing is sacred  just ask ernie fosselius  t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>i hated it  i hate self aware pretentious inan...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>i usually try to be professional and construct...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>if you like me is going to see this in a film ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>this is like a zoology textbook  given that it...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  label_analyzer  \\\n",
       "0    one of the other reviewers has mentioned that ...      1               0   \n",
       "1    a wonderful little production  the filming tec...      1               1   \n",
       "2    i thought this was a wonderful way to spend ti...      1               1   \n",
       "3    basically there s a family where a little boy ...      0               0   \n",
       "4    petter mattei s  love in the time of money  is...      1               1   \n",
       "..                                                 ...    ...             ...   \n",
       "995  nothing is sacred  just ask ernie fosselius  t...      1               1   \n",
       "996  i hated it  i hate self aware pretentious inan...      0               0   \n",
       "997  i usually try to be professional and construct...      0               0   \n",
       "998  if you like me is going to see this in a film ...      0               0   \n",
       "999  this is like a zoology textbook  given that it...      0               0   \n",
       "\n",
       "     label_kmeans  label_lda  label_lda_2  label_pipeline  \n",
       "0               4          0            4               1  \n",
       "1               6          3            1               1  \n",
       "2               2          5            3               1  \n",
       "3               5          6            6               1  \n",
       "4               6          3            6               1  \n",
       "..            ...        ...          ...             ...  \n",
       "995             4          2            4               1  \n",
       "996             5          2            0               0  \n",
       "997             3          1            2               0  \n",
       "998             1          5            3               0  \n",
       "999             4          5            4               0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f2a96",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4b0db",
   "metadata": {},
   "source": [
    "**//Multi Class Labels Scoring\\\\\\**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85e8da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_labels_accuracy_score_logistic_regression(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accurcy score of\",label_name,\"column:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1726e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy score of label_kmeans column: 0.512\n",
      "Accurcy score of label_lda column: 0.612\n",
      "Accurcy score of label_lda_2 column: 0.496\n"
     ]
    }
   ],
   "source": [
    "multi_class_labels_accuracy_score_logistic_regression('label_kmeans')\n",
    "multi_class_labels_accuracy_score_logistic_regression('label_lda')\n",
    "multi_class_labels_accuracy_score_logistic_regression('label_lda_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a531a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_labels_accuracy_score_decision_trees(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    dtc = DecisionTreeClassifier(criterion='gini',max_depth=3, random_state=42)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    y_pred_dt = dtc.predict(X_test)\n",
    "    accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "    print(\"Accuracy_score of\",label_name,\"column:\",accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b52d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of label_kmeans column: 0.328\n",
      "Accuracy_score of label_lda column: 0.288\n",
      "Accuracy_score of label_lda_2 column: 0.448\n"
     ]
    }
   ],
   "source": [
    "multi_class_labels_accuracy_score_decision_trees('label_kmeans')\n",
    "multi_class_labels_accuracy_score_decision_trees('label_lda')\n",
    "multi_class_labels_accuracy_score_decision_trees('label_lda_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f7473df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_labels_accuracy_score_random_forest_classifier(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    rfc = RandomForestClassifier(n_estimators=50, max_features=\"auto\",criterion = 'gini', random_state=44)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred_rfc = rfc.predict(X_test)\n",
    "    accuracy_rfc = accuracy_score(y_test, y_pred_rfc)\n",
    "    print(\"Accuracy_score of\",label_name,\"column:\",accuracy_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92047607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score of label_kmeans column: 0.44\n",
      "Accuracy_score of label_lda column: 0.348\n",
      "Accuracy_score of label_lda_2 column: 0.444\n"
     ]
    }
   ],
   "source": [
    "multi_class_labels_accuracy_score_random_forest_classifier('label_kmeans')\n",
    "multi_class_labels_accuracy_score_random_forest_classifier('label_lda')\n",
    "multi_class_labels_accuracy_score_random_forest_classifier('label_lda_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f1a9f",
   "metadata": {},
   "source": [
    "**From the scores, it can be stated that the algorithms used for topic modeling do not give good enough results or do not fit the data correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1254fe",
   "metadata": {},
   "source": [
    "**//Binary Labels Scoring\\\\\\**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9aaaa",
   "metadata": {},
   "source": [
    "**Accuracy Score and F1 Score of Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8baf8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_of_naive_bayes(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train,y_train)\n",
    "    y_pred_nb = gnb.predict(X_test)\n",
    "    f1_score_nb = f1_score(y_test,y_pred_nb)\n",
    "    accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "    print(\"f1_score_of\",label_name,\"column:\",f1_score_nb,\"accuracy_score:\",label_name,\"column:\",accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fe699fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_of label_analyzer column: 0.7298050139275767 accuracy_score: label_analyzer column: 0.612\n",
      "f1_score_of label column: 0.6160337552742615 accuracy_score: label column: 0.636\n",
      "f1_score_of label_pipeline column: 0.6339622641509434 accuracy_score: label_pipeline column: 0.612\n"
     ]
    }
   ],
   "source": [
    "scores_of_naive_bayes(\"label_analyzer\")\n",
    "scores_of_naive_bayes(\"label\")\n",
    "scores_of_naive_bayes(\"label_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da7bcf",
   "metadata": {},
   "source": [
    "**Accuracy Score and F1 Score of Decision Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b45231c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_of_decision_trees(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    dtc = DecisionTreeClassifier(random_state=0)\n",
    "    dtc.fit(X_train,y_train)\n",
    "    y_pred_dt = dtc.predict(X_test)\n",
    "    f1_score_dt = f1_score(y_test,y_pred_dt)\n",
    "    accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "    print(\"f1_score_of\",label_name,\"column:\",f1_score_dt,\"accuracy_score:\",label_name,\"column:\",accuracy_dt)\n",
    "    print(classification_report(y_test, y_pred_dt),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3973e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_of label_analyzer column: 0.7592592592592593 accuracy_score: label_analyzer column: 0.688\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.56        92\n",
      "           1       0.74      0.78      0.76       158\n",
      "\n",
      "    accuracy                           0.69       250\n",
      "   macro avg       0.66      0.66      0.66       250\n",
      "weighted avg       0.68      0.69      0.68       250\n",
      " \n",
      "\n",
      "f1_score_of label column: 0.7068273092369477 accuracy_score: label column: 0.708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71       124\n",
      "           1       0.72      0.70      0.71       126\n",
      "\n",
      "    accuracy                           0.71       250\n",
      "   macro avg       0.71      0.71      0.71       250\n",
      "weighted avg       0.71      0.71      0.71       250\n",
      " \n",
      "\n",
      "f1_score_of label_pipeline column: 0.6554621848739496 accuracy_score: label_pipeline column: 0.672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       129\n",
      "           1       0.67      0.64      0.66       121\n",
      "\n",
      "    accuracy                           0.67       250\n",
      "   macro avg       0.67      0.67      0.67       250\n",
      "weighted avg       0.67      0.67      0.67       250\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_of_decision_trees(\"label_analyzer\")\n",
    "scores_of_decision_trees(\"label\")\n",
    "scores_of_decision_trees(\"label_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bbcc7",
   "metadata": {},
   "source": [
    "**Accuracy Score and F1 Score of Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb49aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_of_random_forest_classifier(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    rfc = RandomForestClassifier(n_estimators=50, max_features=\"auto\", random_state=44)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred_rfc = rfc.predict(X_test)\n",
    "    f1_score_rfc = f1_score(y_test,y_pred_rfc)\n",
    "    accuracy_rfc = accuracy_score(y_test, y_pred_rfc)\n",
    "    print(\"f1_score_of\",label_name,\"column:\",f1_score_rfc,\"accuracy_score:\",label_name,\"column:\",accuracy_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c44a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_of label_analyzer column: 0.8404255319148937 accuracy_score: label_analyzer column: 0.76\n",
      "f1_score_of label column: 0.7654320987654321 accuracy_score: label column: 0.772\n",
      "f1_score_of label_pipeline column: 0.7306273062730627 accuracy_score: label_pipeline column: 0.708\n"
     ]
    }
   ],
   "source": [
    "scores_of_random_forest_classifier(\"label_analyzer\")\n",
    "scores_of_random_forest_classifier(\"label\")\n",
    "scores_of_random_forest_classifier(\"label_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de6514",
   "metadata": {},
   "source": [
    "**Accuracy Score and F1 Score of Random Gradient Boosting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4dfb93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_of_gradient_boosting_classifier(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train,y_train)\n",
    "    y_pred_gbm = gbc.predict(X_test)\n",
    "    f1_score_gbm = f1_score(y_test,y_pred_gbm)\n",
    "    accuracy_gbm = accuracy_score(y_test, y_pred_gbm)\n",
    "    print(\"f1_score_of\",label_name,\"column:\",f1_score_gbm,\"accuracy_score:\",label_name,\"column:\",accuracy_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f2bb5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_of label_analyzer column: 0.8022922636103151 accuracy_score: label_analyzer column: 0.724\n",
      "f1_score_of label column: 0.7452471482889734 accuracy_score: label column: 0.732\n",
      "f1_score_of label_pipeline column: 0.7741935483870968 accuracy_score: label_pipeline column: 0.748\n"
     ]
    }
   ],
   "source": [
    "scores_of_gradient_boosting_classifier(\"label_analyzer\")\n",
    "scores_of_gradient_boosting_classifier(\"label\")\n",
    "scores_of_gradient_boosting_classifier(\"label_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2ce7e",
   "metadata": {},
   "source": [
    "**It can be said that binary classes are better created than multi class labels. And it can be seen that the classes created with \"SentimentIntensityAnalyzer\" in particular give good results in all models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e7034d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = text_vectorized\n",
    "y = df[\"label_analyzer\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_features=\"auto\", random_state=44)\n",
    "rfc.fit(X_train, y_train)\n",
    "plot_confusion_matrix(rfc, X_test, y_test) \n",
    "plt.title(\"Confusion Matrix of Random Forest Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ac1ee",
   "metadata": {},
   "source": [
    "**There have been made 30(+-) positive predicted and true, 50(+-) positive predicted and false, 10(+-) negative predicted and false, 150(+-) negative predicted and true interpretation here we made with the Random Forest Classification model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb6163f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_proba = rfc.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.title(\"ROC Curve of Random Forest Classifier\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a59c67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "919c8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_binary_class(label_name):\n",
    "    X = text_vectorized\n",
    "    y = df[label_name]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(32, activation='relu',input_dim=15763))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(np.array(X_train),np.array(y_train),validation_split=0.25, epochs=20, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "093899ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.6922 - accuracy: 0.5302 - val_loss: 0.6889 - val_accuracy: 0.6223\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6748 - accuracy: 0.6851 - val_loss: 0.6823 - val_accuracy: 0.6968\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6526 - accuracy: 0.7740 - val_loss: 0.6686 - val_accuracy: 0.7287\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6248 - accuracy: 0.8203 - val_loss: 0.6489 - val_accuracy: 0.7447\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5778 - accuracy: 0.8719 - val_loss: 0.6284 - val_accuracy: 0.7553\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5207 - accuracy: 0.8861 - val_loss: 0.6036 - val_accuracy: 0.7872\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4802 - accuracy: 0.8950 - val_loss: 0.5808 - val_accuracy: 0.8032\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4311 - accuracy: 0.9128 - val_loss: 0.5593 - val_accuracy: 0.8351\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3791 - accuracy: 0.9288 - val_loss: 0.5375 - val_accuracy: 0.8245\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3353 - accuracy: 0.9377 - val_loss: 0.5141 - val_accuracy: 0.8298\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3053 - accuracy: 0.9502 - val_loss: 0.4938 - val_accuracy: 0.8404\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2510 - accuracy: 0.9733 - val_loss: 0.4754 - val_accuracy: 0.8351\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2377 - accuracy: 0.9644 - val_loss: 0.4594 - val_accuracy: 0.8351\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1938 - accuracy: 0.9769 - val_loss: 0.4477 - val_accuracy: 0.8351\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1819 - accuracy: 0.9786 - val_loss: 0.4399 - val_accuracy: 0.8351\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1476 - accuracy: 0.9822 - val_loss: 0.4356 - val_accuracy: 0.8351\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1351 - accuracy: 0.9858 - val_loss: 0.4336 - val_accuracy: 0.8351\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1206 - accuracy: 0.9822 - val_loss: 0.4317 - val_accuracy: 0.8351\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1066 - accuracy: 0.9875 - val_loss: 0.4329 - val_accuracy: 0.8298\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0808 - accuracy: 0.9982 - val_loss: 0.4352 - val_accuracy: 0.8351\n"
     ]
    }
   ],
   "source": [
    "neural_network_binary_class(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "962152ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6995 - accuracy: 0.5231 - val_loss: 0.6871 - val_accuracy: 0.5053\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6592 - accuracy: 0.5979 - val_loss: 0.6789 - val_accuracy: 0.5638\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6254 - accuracy: 0.6512 - val_loss: 0.6699 - val_accuracy: 0.5372\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5843 - accuracy: 0.6922 - val_loss: 0.6718 - val_accuracy: 0.5266\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5497 - accuracy: 0.6886 - val_loss: 0.6722 - val_accuracy: 0.5479\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5227 - accuracy: 0.7420 - val_loss: 0.6611 - val_accuracy: 0.5851\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4811 - accuracy: 0.8274 - val_loss: 0.6560 - val_accuracy: 0.6223\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4431 - accuracy: 0.8808 - val_loss: 0.6597 - val_accuracy: 0.6223\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4223 - accuracy: 0.8861 - val_loss: 0.6699 - val_accuracy: 0.6277\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3837 - accuracy: 0.9324 - val_loss: 0.6802 - val_accuracy: 0.6383\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3631 - accuracy: 0.9431 - val_loss: 0.6729 - val_accuracy: 0.6489\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3391 - accuracy: 0.9484 - val_loss: 0.6571 - val_accuracy: 0.6862\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3050 - accuracy: 0.9662 - val_loss: 0.6398 - val_accuracy: 0.6862\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2900 - accuracy: 0.9644 - val_loss: 0.6224 - val_accuracy: 0.6968\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2521 - accuracy: 0.9769 - val_loss: 0.5988 - val_accuracy: 0.7128\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2302 - accuracy: 0.9804 - val_loss: 0.5677 - val_accuracy: 0.7766\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2137 - accuracy: 0.9769 - val_loss: 0.5362 - val_accuracy: 0.7819\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1885 - accuracy: 0.9858 - val_loss: 0.5145 - val_accuracy: 0.8085\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1790 - accuracy: 0.9858 - val_loss: 0.5135 - val_accuracy: 0.8085\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1457 - accuracy: 0.9964 - val_loss: 0.5218 - val_accuracy: 0.8032\n"
     ]
    }
   ],
   "source": [
    "neural_network_binary_class(\"label_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4c0e78a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6960 - accuracy: 0.4733 - val_loss: 0.6909 - val_accuracy: 0.5745\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6814 - accuracy: 0.6370 - val_loss: 0.6864 - val_accuracy: 0.6011\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6670 - accuracy: 0.6441 - val_loss: 0.6822 - val_accuracy: 0.5904\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6460 - accuracy: 0.6566 - val_loss: 0.6804 - val_accuracy: 0.5904\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6244 - accuracy: 0.6797 - val_loss: 0.6784 - val_accuracy: 0.5904\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5939 - accuracy: 0.7420 - val_loss: 0.6749 - val_accuracy: 0.5904\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5645 - accuracy: 0.7651 - val_loss: 0.6695 - val_accuracy: 0.5957\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5349 - accuracy: 0.8043 - val_loss: 0.6600 - val_accuracy: 0.6011\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4913 - accuracy: 0.8452 - val_loss: 0.6568 - val_accuracy: 0.6170\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4578 - accuracy: 0.8630 - val_loss: 0.6621 - val_accuracy: 0.6170\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4226 - accuracy: 0.8897 - val_loss: 0.6617 - val_accuracy: 0.6170\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3915 - accuracy: 0.8879 - val_loss: 0.6533 - val_accuracy: 0.6117\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3656 - accuracy: 0.9075 - val_loss: 0.6451 - val_accuracy: 0.6330\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3188 - accuracy: 0.9413 - val_loss: 0.6450 - val_accuracy: 0.6277\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2771 - accuracy: 0.9555 - val_loss: 0.6486 - val_accuracy: 0.6436\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2629 - accuracy: 0.9662 - val_loss: 0.6658 - val_accuracy: 0.6489\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2236 - accuracy: 0.9626 - val_loss: 0.6824 - val_accuracy: 0.6383\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1907 - accuracy: 0.9858 - val_loss: 0.6895 - val_accuracy: 0.6596\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1691 - accuracy: 0.9911 - val_loss: 0.7027 - val_accuracy: 0.6596\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.1427 - accuracy: 0.9875 - val_loss: 0.7301 - val_accuracy: 0.6596\n"
     ]
    }
   ],
   "source": [
    "neural_network_binary_class(\"label_analyzer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4a55eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_multi_class(label_name): \n",
    "    \n",
    "    X = np.array(text_vectorized)\n",
    "    y = np_utils.to_categorical(df[label_name])\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, activation='relu',input_dim=15763))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(X, y, epochs=200, batch_size=10, shuffle=True, validation_split=0.2, verbose=1)\n",
    "    print(\"\\n\")\n",
    "    model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d0d37ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.9267 - accuracy: 0.2075 - val_loss: 1.9006 - val_accuracy: 0.2100\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.8167 - accuracy: 0.2675 - val_loss: 1.8304 - val_accuracy: 0.2350\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.6926 - accuracy: 0.3275 - val_loss: 1.7642 - val_accuracy: 0.2600\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.5608 - accuracy: 0.3938 - val_loss: 1.7199 - val_accuracy: 0.2600\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.4665 - accuracy: 0.4288 - val_loss: 1.7001 - val_accuracy: 0.2800\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.3929 - accuracy: 0.4450 - val_loss: 1.6974 - val_accuracy: 0.2750\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.2881 - accuracy: 0.4863 - val_loss: 1.6547 - val_accuracy: 0.2750\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1994 - accuracy: 0.5125 - val_loss: 1.6453 - val_accuracy: 0.3250\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1375 - accuracy: 0.5263 - val_loss: 1.6359 - val_accuracy: 0.3650\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0675 - accuracy: 0.5562 - val_loss: 1.6210 - val_accuracy: 0.3800\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9982 - accuracy: 0.5888 - val_loss: 1.6103 - val_accuracy: 0.3700\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9799 - accuracy: 0.5850 - val_loss: 1.6066 - val_accuracy: 0.3750\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9655 - accuracy: 0.6050 - val_loss: 1.6266 - val_accuracy: 0.4150\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8784 - accuracy: 0.6600 - val_loss: 1.5995 - val_accuracy: 0.4150\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8858 - accuracy: 0.6513 - val_loss: 1.5662 - val_accuracy: 0.4400\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8044 - accuracy: 0.6950 - val_loss: 1.5940 - val_accuracy: 0.4500\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8005 - accuracy: 0.7100 - val_loss: 1.5943 - val_accuracy: 0.4700\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7776 - accuracy: 0.7050 - val_loss: 1.6551 - val_accuracy: 0.4700\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7204 - accuracy: 0.7425 - val_loss: 1.6569 - val_accuracy: 0.4450\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7284 - accuracy: 0.7412 - val_loss: 1.6432 - val_accuracy: 0.4800\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7152 - accuracy: 0.7563 - val_loss: 1.6144 - val_accuracy: 0.4900\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.7875 - val_loss: 1.6162 - val_accuracy: 0.4850\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.7763 - val_loss: 1.6282 - val_accuracy: 0.4850\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.7837 - val_loss: 1.6695 - val_accuracy: 0.4650\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.8087 - val_loss: 1.8769 - val_accuracy: 0.4300\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5867 - accuracy: 0.8100 - val_loss: 1.7488 - val_accuracy: 0.4900\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5553 - accuracy: 0.8225 - val_loss: 1.7496 - val_accuracy: 0.4650\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.8087 - val_loss: 1.6897 - val_accuracy: 0.5050\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.8263 - val_loss: 1.8176 - val_accuracy: 0.4450\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.7950 - val_loss: 1.8038 - val_accuracy: 0.4600\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.8238 - val_loss: 1.9090 - val_accuracy: 0.4600\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8338 - val_loss: 1.8862 - val_accuracy: 0.4450\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.8363 - val_loss: 1.7964 - val_accuracy: 0.4600\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.8300 - val_loss: 1.9018 - val_accuracy: 0.4600\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.8487 - val_loss: 2.0101 - val_accuracy: 0.4400\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.8487 - val_loss: 1.9449 - val_accuracy: 0.4750\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.8562 - val_loss: 1.9920 - val_accuracy: 0.4650\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.8438 - val_loss: 2.0988 - val_accuracy: 0.4450\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.8338 - val_loss: 2.0287 - val_accuracy: 0.4550\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.8612 - val_loss: 2.1354 - val_accuracy: 0.4600\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.8612 - val_loss: 1.9979 - val_accuracy: 0.4750\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8687 - val_loss: 1.9996 - val_accuracy: 0.4700\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8637 - val_loss: 1.9281 - val_accuracy: 0.4900\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8637 - val_loss: 2.0306 - val_accuracy: 0.4850\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8875 - val_loss: 2.1733 - val_accuracy: 0.4800\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8813 - val_loss: 2.1755 - val_accuracy: 0.4600\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8850 - val_loss: 2.1430 - val_accuracy: 0.4650\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.8788 - val_loss: 2.0721 - val_accuracy: 0.4700\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8850 - val_loss: 2.2382 - val_accuracy: 0.4900\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8725 - val_loss: 2.2656 - val_accuracy: 0.4700\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.9150 - val_loss: 2.2714 - val_accuracy: 0.4800\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8775 - val_loss: 2.0746 - val_accuracy: 0.4600\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8975 - val_loss: 2.2237 - val_accuracy: 0.4800\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8888 - val_loss: 2.3498 - val_accuracy: 0.4550\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8938 - val_loss: 2.4089 - val_accuracy: 0.4650\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3635 - accuracy: 0.8925 - val_loss: 2.3557 - val_accuracy: 0.4550\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8963 - val_loss: 2.3481 - val_accuracy: 0.4450\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.8925 - val_loss: 2.5656 - val_accuracy: 0.4550\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8900 - val_loss: 2.2865 - val_accuracy: 0.4650\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8813 - val_loss: 2.3800 - val_accuracy: 0.4700\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8825 - val_loss: 2.3635 - val_accuracy: 0.4700\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8838 - val_loss: 2.3814 - val_accuracy: 0.4650\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.9038 - val_loss: 2.3344 - val_accuracy: 0.4600\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8925 - val_loss: 2.4152 - val_accuracy: 0.4700\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8938 - val_loss: 2.6492 - val_accuracy: 0.4550\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.9000 - val_loss: 2.9266 - val_accuracy: 0.4550\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.9100 - val_loss: 2.6625 - val_accuracy: 0.4900\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.9038 - val_loss: 2.5937 - val_accuracy: 0.5050\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.9087 - val_loss: 3.0280 - val_accuracy: 0.4750\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.9025 - val_loss: 2.8973 - val_accuracy: 0.4900\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.9162 - val_loss: 2.7315 - val_accuracy: 0.4700\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.9150 - val_loss: 2.6512 - val_accuracy: 0.5050\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8888 - val_loss: 2.9973 - val_accuracy: 0.4750\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8975 - val_loss: 2.9705 - val_accuracy: 0.4850\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.9225 - val_loss: 2.7913 - val_accuracy: 0.4850\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8900 - val_loss: 2.8835 - val_accuracy: 0.5050\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2997 - accuracy: 0.9038 - val_loss: 2.9765 - val_accuracy: 0.4850\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.9100 - val_loss: 2.8058 - val_accuracy: 0.4800\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.9062 - val_loss: 2.7333 - val_accuracy: 0.5000\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.9137 - val_loss: 2.8290 - val_accuracy: 0.5150\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.9212 - val_loss: 2.8870 - val_accuracy: 0.4900\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.9112 - val_loss: 3.0980 - val_accuracy: 0.5050\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8975 - val_loss: 3.1789 - val_accuracy: 0.4850\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.9250 - val_loss: 3.0350 - val_accuracy: 0.4850\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2581 - accuracy: 0.9200 - val_loss: 3.1702 - val_accuracy: 0.4600\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9175 - val_loss: 3.0677 - val_accuracy: 0.4800\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.9137 - val_loss: 3.3042 - val_accuracy: 0.4700\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.9137 - val_loss: 3.4782 - val_accuracy: 0.4600\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.9175 - val_loss: 3.4165 - val_accuracy: 0.4850\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.9112 - val_loss: 3.3110 - val_accuracy: 0.4800\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.9150 - val_loss: 3.4421 - val_accuracy: 0.4650\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.9200 - val_loss: 3.5486 - val_accuracy: 0.4750\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.9137 - val_loss: 3.4923 - val_accuracy: 0.4600\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.9175 - val_loss: 3.6507 - val_accuracy: 0.4550\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.9038 - val_loss: 3.8508 - val_accuracy: 0.4700\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.9125 - val_loss: 3.6501 - val_accuracy: 0.4550\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.9262 - val_loss: 3.6596 - val_accuracy: 0.4500\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.9175 - val_loss: 3.4300 - val_accuracy: 0.4550\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3119 - accuracy: 0.9250 - val_loss: 3.3090 - val_accuracy: 0.4650\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.9050 - val_loss: 3.3215 - val_accuracy: 0.4800\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.9050 - val_loss: 3.1981 - val_accuracy: 0.4800\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.9200 - val_loss: 3.3497 - val_accuracy: 0.4750\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9312 - val_loss: 3.7151 - val_accuracy: 0.4500\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.9162 - val_loss: 4.0632 - val_accuracy: 0.4550\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.9150 - val_loss: 3.7952 - val_accuracy: 0.4700\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.9362 - val_loss: 3.8459 - val_accuracy: 0.4650\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.9275 - val_loss: 4.0159 - val_accuracy: 0.4350\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.9187 - val_loss: 3.4163 - val_accuracy: 0.4650\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2325 - accuracy: 0.9250 - val_loss: 3.5431 - val_accuracy: 0.4500\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.9187 - val_loss: 3.4568 - val_accuracy: 0.4500\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2227 - accuracy: 0.9337 - val_loss: 3.5763 - val_accuracy: 0.4600\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.9112 - val_loss: 3.5905 - val_accuracy: 0.4600\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.9275 - val_loss: 3.6077 - val_accuracy: 0.4700\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.9275 - val_loss: 3.8446 - val_accuracy: 0.4600\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.9187 - val_loss: 3.7135 - val_accuracy: 0.4500\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.9262 - val_loss: 3.8560 - val_accuracy: 0.4650\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.9400 - val_loss: 4.0290 - val_accuracy: 0.4400\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9287 - val_loss: 3.8386 - val_accuracy: 0.4450\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9425 - val_loss: 4.0292 - val_accuracy: 0.4600\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.9162 - val_loss: 4.0837 - val_accuracy: 0.4450\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.9112 - val_loss: 4.0511 - val_accuracy: 0.4350\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.9250 - val_loss: 4.2048 - val_accuracy: 0.4450\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.9062 - val_loss: 4.1599 - val_accuracy: 0.4550\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.9275 - val_loss: 3.9771 - val_accuracy: 0.4650\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.9212 - val_loss: 3.7643 - val_accuracy: 0.4450\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9237 - val_loss: 3.8598 - val_accuracy: 0.4700\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9388 - val_loss: 3.6365 - val_accuracy: 0.4650\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9275 - val_loss: 3.7859 - val_accuracy: 0.4550\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.9212 - val_loss: 3.8036 - val_accuracy: 0.4550\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.9237 - val_loss: 4.0084 - val_accuracy: 0.4400\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9312 - val_loss: 3.8992 - val_accuracy: 0.4450\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.9225 - val_loss: 4.0753 - val_accuracy: 0.4600\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2187 - accuracy: 0.9362 - val_loss: 4.2012 - val_accuracy: 0.4550\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9425 - val_loss: 4.3699 - val_accuracy: 0.4600\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.9287 - val_loss: 4.2705 - val_accuracy: 0.4650\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9375 - val_loss: 4.4603 - val_accuracy: 0.4600\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2213 - accuracy: 0.9413 - val_loss: 4.3640 - val_accuracy: 0.4650\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.9250 - val_loss: 4.4306 - val_accuracy: 0.4550\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9262 - val_loss: 4.6237 - val_accuracy: 0.4700\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.9162 - val_loss: 4.7387 - val_accuracy: 0.4450\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2111 - accuracy: 0.9400 - val_loss: 4.5228 - val_accuracy: 0.4550\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9350 - val_loss: 4.6382 - val_accuracy: 0.4550\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.9237 - val_loss: 4.5087 - val_accuracy: 0.4450\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1706 - accuracy: 0.9550 - val_loss: 4.4836 - val_accuracy: 0.4500\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.9275 - val_loss: 4.7777 - val_accuracy: 0.4550\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2138 - accuracy: 0.9375 - val_loss: 4.6349 - val_accuracy: 0.4600\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9350 - val_loss: 4.4305 - val_accuracy: 0.4600\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9425 - val_loss: 4.3683 - val_accuracy: 0.4700\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2136 - accuracy: 0.9388 - val_loss: 4.6333 - val_accuracy: 0.4550\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9325 - val_loss: 4.5919 - val_accuracy: 0.4650\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.9312 - val_loss: 5.0935 - val_accuracy: 0.4700\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9375 - val_loss: 5.3263 - val_accuracy: 0.4650\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.9187 - val_loss: 5.1884 - val_accuracy: 0.4550\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.9375 - val_loss: 4.8654 - val_accuracy: 0.4500\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.9375 - val_loss: 4.9881 - val_accuracy: 0.4500\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1881 - accuracy: 0.9488 - val_loss: 4.8296 - val_accuracy: 0.4500\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.9513 - val_loss: 5.1015 - val_accuracy: 0.4650\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.9250 - val_loss: 5.3498 - val_accuracy: 0.4700\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2519 - accuracy: 0.9275 - val_loss: 5.2537 - val_accuracy: 0.4700\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9300 - val_loss: 5.3962 - val_accuracy: 0.4500\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1977 - accuracy: 0.9438 - val_loss: 5.3604 - val_accuracy: 0.4600\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.9300 - val_loss: 5.4620 - val_accuracy: 0.4550\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9438 - val_loss: 5.4961 - val_accuracy: 0.4650\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.9187 - val_loss: 5.4657 - val_accuracy: 0.4450\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.9225 - val_loss: 5.0052 - val_accuracy: 0.4550\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9312 - val_loss: 5.1516 - val_accuracy: 0.4600\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9350 - val_loss: 5.3238 - val_accuracy: 0.4500\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9312 - val_loss: 5.2309 - val_accuracy: 0.4600\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9350 - val_loss: 5.0564 - val_accuracy: 0.4600\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1842 - accuracy: 0.9463 - val_loss: 4.9674 - val_accuracy: 0.4650\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9413 - val_loss: 5.4401 - val_accuracy: 0.4700\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1805 - accuracy: 0.9475 - val_loss: 5.3228 - val_accuracy: 0.4600\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9375 - val_loss: 5.3884 - val_accuracy: 0.4600\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9500 - val_loss: 5.3358 - val_accuracy: 0.4500\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9438 - val_loss: 5.2340 - val_accuracy: 0.4600\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.9337 - val_loss: 5.0312 - val_accuracy: 0.4600\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9275 - val_loss: 5.1656 - val_accuracy: 0.4700\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9350 - val_loss: 5.2009 - val_accuracy: 0.4650\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9375 - val_loss: 5.4831 - val_accuracy: 0.4450\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.9212 - val_loss: 5.2662 - val_accuracy: 0.4550\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9350 - val_loss: 5.7480 - val_accuracy: 0.4500\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2208 - accuracy: 0.9500 - val_loss: 5.9358 - val_accuracy: 0.4450\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2460 - accuracy: 0.9400 - val_loss: 5.2401 - val_accuracy: 0.4700\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9450 - val_loss: 5.2334 - val_accuracy: 0.4650\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.9375 - val_loss: 5.4339 - val_accuracy: 0.4600\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9550 - val_loss: 6.1792 - val_accuracy: 0.4500\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9362 - val_loss: 6.2305 - val_accuracy: 0.4550\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9350 - val_loss: 6.2764 - val_accuracy: 0.4550\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.9400 - val_loss: 6.5188 - val_accuracy: 0.4500\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.9450 - val_loss: 6.8665 - val_accuracy: 0.4400\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9488 - val_loss: 6.6604 - val_accuracy: 0.4500\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9362 - val_loss: 6.6383 - val_accuracy: 0.4300\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9362 - val_loss: 6.6103 - val_accuracy: 0.4550\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1932 - accuracy: 0.9475 - val_loss: 6.4459 - val_accuracy: 0.4400\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9438 - val_loss: 6.6647 - val_accuracy: 0.4350\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9312 - val_loss: 5.8145 - val_accuracy: 0.4600\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1969 - accuracy: 0.9413 - val_loss: 6.0189 - val_accuracy: 0.4550\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9463 - val_loss: 5.8977 - val_accuracy: 0.4650\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9450 - val_loss: 6.4198 - val_accuracy: 0.4400\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9325 - val_loss: 6.8154 - val_accuracy: 0.4400\n",
      "\n",
      "\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.5496 - accuracy: 0.8560\n"
     ]
    }
   ],
   "source": [
    "neural_network_multi_class(\"label_kmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ff21bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.9184 - accuracy: 0.2150 - val_loss: 1.8994 - val_accuracy: 0.2250\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.8468 - accuracy: 0.2700 - val_loss: 1.8697 - val_accuracy: 0.3050\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.7512 - accuracy: 0.3088 - val_loss: 1.8287 - val_accuracy: 0.3200\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.7008 - accuracy: 0.3088 - val_loss: 1.7945 - val_accuracy: 0.3250\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.6044 - accuracy: 0.3363 - val_loss: 1.7467 - val_accuracy: 0.3600\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.5161 - accuracy: 0.3738 - val_loss: 1.7033 - val_accuracy: 0.3650\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.4338 - accuracy: 0.4263 - val_loss: 1.6668 - val_accuracy: 0.4050\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.3440 - accuracy: 0.4762 - val_loss: 1.6235 - val_accuracy: 0.4000\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.2908 - accuracy: 0.4975 - val_loss: 1.5864 - val_accuracy: 0.4050\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.2265 - accuracy: 0.5437 - val_loss: 1.5702 - val_accuracy: 0.4050\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1635 - accuracy: 0.5500 - val_loss: 1.5304 - val_accuracy: 0.4250\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1211 - accuracy: 0.5838 - val_loss: 1.5107 - val_accuracy: 0.4600\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0309 - accuracy: 0.5925 - val_loss: 1.5005 - val_accuracy: 0.4450\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9997 - accuracy: 0.6100 - val_loss: 1.5042 - val_accuracy: 0.4350\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9669 - accuracy: 0.6325 - val_loss: 1.5348 - val_accuracy: 0.4150\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9745 - accuracy: 0.6237 - val_loss: 1.6068 - val_accuracy: 0.4050\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9322 - accuracy: 0.6463 - val_loss: 1.5201 - val_accuracy: 0.4100\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9113 - accuracy: 0.6425 - val_loss: 1.5078 - val_accuracy: 0.4350\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9063 - accuracy: 0.6587 - val_loss: 1.5394 - val_accuracy: 0.4400\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8220 - accuracy: 0.6925 - val_loss: 1.5551 - val_accuracy: 0.4000\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7785 - accuracy: 0.7050 - val_loss: 1.5892 - val_accuracy: 0.4350\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8216 - accuracy: 0.6800 - val_loss: 1.5448 - val_accuracy: 0.4300\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7539 - accuracy: 0.6913 - val_loss: 1.6036 - val_accuracy: 0.4350\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7779 - accuracy: 0.7075 - val_loss: 1.5754 - val_accuracy: 0.4200\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7665 - accuracy: 0.7075 - val_loss: 1.5088 - val_accuracy: 0.4300\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7667 - accuracy: 0.6938 - val_loss: 1.5292 - val_accuracy: 0.4000\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7412 - accuracy: 0.7088 - val_loss: 1.6326 - val_accuracy: 0.4100\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7234 - accuracy: 0.6925 - val_loss: 1.6012 - val_accuracy: 0.4400\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7368 - accuracy: 0.7050 - val_loss: 1.6753 - val_accuracy: 0.4350\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7691 - accuracy: 0.7138 - val_loss: 1.6671 - val_accuracy: 0.4200\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.7250 - val_loss: 1.6036 - val_accuracy: 0.4250\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6953 - accuracy: 0.7412 - val_loss: 1.6807 - val_accuracy: 0.4150\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.7287 - val_loss: 1.6008 - val_accuracy: 0.4550\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6799 - accuracy: 0.7450 - val_loss: 1.6053 - val_accuracy: 0.4400\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6146 - accuracy: 0.7675 - val_loss: 1.6830 - val_accuracy: 0.4700\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6114 - accuracy: 0.7513 - val_loss: 1.7362 - val_accuracy: 0.4400\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.7375 - val_loss: 1.6658 - val_accuracy: 0.4350\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6502 - accuracy: 0.7437 - val_loss: 1.8377 - val_accuracy: 0.4150\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.7638 - val_loss: 1.7669 - val_accuracy: 0.4350\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.7475 - val_loss: 1.7031 - val_accuracy: 0.4250\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7700 - val_loss: 1.8290 - val_accuracy: 0.4200\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.7450 - val_loss: 1.8470 - val_accuracy: 0.4250\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.7625 - val_loss: 1.9007 - val_accuracy: 0.4100\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6267 - accuracy: 0.7575 - val_loss: 1.9621 - val_accuracy: 0.4000\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5412 - accuracy: 0.7850 - val_loss: 1.8661 - val_accuracy: 0.4100\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7775 - val_loss: 1.9830 - val_accuracy: 0.4400\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.7500 - val_loss: 1.9446 - val_accuracy: 0.4350\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7613 - val_loss: 1.9280 - val_accuracy: 0.4400\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7625 - val_loss: 2.1035 - val_accuracy: 0.4300\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.7775 - val_loss: 2.3105 - val_accuracy: 0.4250\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7763 - val_loss: 2.0568 - val_accuracy: 0.4250\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7675 - val_loss: 2.0820 - val_accuracy: 0.4400\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7825 - val_loss: 2.0890 - val_accuracy: 0.4350\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7837 - val_loss: 2.3503 - val_accuracy: 0.4200\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.7588 - val_loss: 2.2070 - val_accuracy: 0.4300\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7812 - val_loss: 2.2021 - val_accuracy: 0.4300\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6034 - accuracy: 0.7738 - val_loss: 2.1006 - val_accuracy: 0.4250\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7800 - val_loss: 2.1708 - val_accuracy: 0.4250\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7875 - val_loss: 2.3384 - val_accuracy: 0.4500\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7738 - val_loss: 1.9420 - val_accuracy: 0.4500\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7875 - val_loss: 2.0242 - val_accuracy: 0.4350\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7788 - val_loss: 2.1785 - val_accuracy: 0.4350\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.8138 - val_loss: 2.2048 - val_accuracy: 0.4300\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7887 - val_loss: 2.2340 - val_accuracy: 0.4150\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7912 - val_loss: 2.3128 - val_accuracy: 0.4200\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5315 - accuracy: 0.7812 - val_loss: 2.2341 - val_accuracy: 0.4250\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.7825 - val_loss: 2.2045 - val_accuracy: 0.4100\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.7837 - val_loss: 2.3940 - val_accuracy: 0.4150\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7987 - val_loss: 2.2841 - val_accuracy: 0.4100\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7887 - val_loss: 2.3885 - val_accuracy: 0.4150\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7850 - val_loss: 2.4043 - val_accuracy: 0.4400\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7912 - val_loss: 2.4401 - val_accuracy: 0.4350\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.7788 - val_loss: 2.4124 - val_accuracy: 0.4400\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7912 - val_loss: 2.4141 - val_accuracy: 0.4400\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4554 - accuracy: 0.8100 - val_loss: 2.6876 - val_accuracy: 0.4250\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.8037 - val_loss: 2.9459 - val_accuracy: 0.4050\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7975 - val_loss: 2.8054 - val_accuracy: 0.4250\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8000 - val_loss: 2.7039 - val_accuracy: 0.4350\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7950 - val_loss: 2.6641 - val_accuracy: 0.4250\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8012 - val_loss: 2.4458 - val_accuracy: 0.4300\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7850 - val_loss: 2.4459 - val_accuracy: 0.4300\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7962 - val_loss: 2.5451 - val_accuracy: 0.4300\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.8037 - val_loss: 2.6585 - val_accuracy: 0.4250\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7900 - val_loss: 2.8279 - val_accuracy: 0.4350\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7987 - val_loss: 2.8049 - val_accuracy: 0.4400\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.8037 - val_loss: 2.7972 - val_accuracy: 0.4250\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7962 - val_loss: 3.0342 - val_accuracy: 0.3950\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.8025 - val_loss: 2.7538 - val_accuracy: 0.4250\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8000 - val_loss: 2.9661 - val_accuracy: 0.4300\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7900 - val_loss: 2.8997 - val_accuracy: 0.4300\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.8087 - val_loss: 2.8722 - val_accuracy: 0.4200\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.8050 - val_loss: 3.1303 - val_accuracy: 0.4200\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.8112 - val_loss: 3.2044 - val_accuracy: 0.4200\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4580 - accuracy: 0.7987 - val_loss: 3.4394 - val_accuracy: 0.4050\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8200 - val_loss: 3.2968 - val_accuracy: 0.4150\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8138 - val_loss: 3.0884 - val_accuracy: 0.4250\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.8087 - val_loss: 2.9492 - val_accuracy: 0.4400\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8250 - val_loss: 2.8047 - val_accuracy: 0.4250\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8112 - val_loss: 3.1067 - val_accuracy: 0.4250\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.8225 - val_loss: 2.9214 - val_accuracy: 0.4250\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4764 - accuracy: 0.8025 - val_loss: 3.0018 - val_accuracy: 0.4350\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.8075 - val_loss: 2.8645 - val_accuracy: 0.4200\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.8087 - val_loss: 2.7522 - val_accuracy: 0.4550\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.8112 - val_loss: 2.9775 - val_accuracy: 0.4300\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8275 - val_loss: 2.7863 - val_accuracy: 0.4400\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.8075 - val_loss: 3.2906 - val_accuracy: 0.4100\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8150 - val_loss: 2.9980 - val_accuracy: 0.4450\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8100 - val_loss: 3.3567 - val_accuracy: 0.4200\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.8238 - val_loss: 3.3545 - val_accuracy: 0.4250\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.8012 - val_loss: 3.3974 - val_accuracy: 0.4100\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8350 - val_loss: 3.1238 - val_accuracy: 0.4500\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8112 - val_loss: 3.1788 - val_accuracy: 0.4300\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8138 - val_loss: 3.2102 - val_accuracy: 0.4350\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.8125 - val_loss: 3.2402 - val_accuracy: 0.4300\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.8275 - val_loss: 2.9581 - val_accuracy: 0.4500\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.8188 - val_loss: 2.8645 - val_accuracy: 0.4550\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8413 - val_loss: 3.2140 - val_accuracy: 0.4400\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8175 - val_loss: 3.2176 - val_accuracy: 0.4350\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4585 - accuracy: 0.8338 - val_loss: 3.0303 - val_accuracy: 0.4350\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.8225 - val_loss: 2.9465 - val_accuracy: 0.4250\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.8175 - val_loss: 2.9696 - val_accuracy: 0.4350\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.8138 - val_loss: 2.9452 - val_accuracy: 0.4500\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8325 - val_loss: 3.0808 - val_accuracy: 0.4400\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8250 - val_loss: 3.2584 - val_accuracy: 0.4300\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8150 - val_loss: 3.3229 - val_accuracy: 0.4450\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8175 - val_loss: 3.4569 - val_accuracy: 0.4350\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.8238 - val_loss: 3.5389 - val_accuracy: 0.4400\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.8138 - val_loss: 3.5538 - val_accuracy: 0.4300\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8263 - val_loss: 3.3690 - val_accuracy: 0.4250\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.8200 - val_loss: 3.3540 - val_accuracy: 0.4300\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.8200 - val_loss: 3.3683 - val_accuracy: 0.4300\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8363 - val_loss: 3.1772 - val_accuracy: 0.4200\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8275 - val_loss: 3.2386 - val_accuracy: 0.4150\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8363 - val_loss: 3.1174 - val_accuracy: 0.4550\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.8350 - val_loss: 3.3819 - val_accuracy: 0.4300\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4270 - accuracy: 0.8288 - val_loss: 3.2526 - val_accuracy: 0.4250\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8163 - val_loss: 3.3035 - val_accuracy: 0.4250\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8400 - val_loss: 3.5488 - val_accuracy: 0.4400\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.8112 - val_loss: 3.5995 - val_accuracy: 0.4300\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8150 - val_loss: 3.4896 - val_accuracy: 0.4250\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8587 - val_loss: 3.5929 - val_accuracy: 0.4000\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8275 - val_loss: 3.8072 - val_accuracy: 0.4150\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8400 - val_loss: 3.7283 - val_accuracy: 0.4250\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8313 - val_loss: 3.5605 - val_accuracy: 0.4300\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8263 - val_loss: 3.6232 - val_accuracy: 0.4250\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.8188 - val_loss: 3.4427 - val_accuracy: 0.4250\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8363 - val_loss: 3.4366 - val_accuracy: 0.4450\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8275 - val_loss: 3.4936 - val_accuracy: 0.4500\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8225 - val_loss: 3.4833 - val_accuracy: 0.4550\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8288 - val_loss: 3.6081 - val_accuracy: 0.4500\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.8138 - val_loss: 3.5309 - val_accuracy: 0.4400\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8388 - val_loss: 3.7737 - val_accuracy: 0.4400\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8350 - val_loss: 3.9270 - val_accuracy: 0.4400\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8338 - val_loss: 3.7255 - val_accuracy: 0.4500\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8350 - val_loss: 3.7404 - val_accuracy: 0.4500\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8175 - val_loss: 3.6641 - val_accuracy: 0.4500\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4484 - accuracy: 0.8325 - val_loss: 3.6991 - val_accuracy: 0.4550\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8625 - val_loss: 3.9438 - val_accuracy: 0.4300\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8425 - val_loss: 4.3354 - val_accuracy: 0.4400\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8325 - val_loss: 4.2709 - val_accuracy: 0.4250\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8625 - val_loss: 4.2063 - val_accuracy: 0.4450\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8425 - val_loss: 4.4737 - val_accuracy: 0.4100\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8400 - val_loss: 4.3558 - val_accuracy: 0.4450\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8550 - val_loss: 4.5679 - val_accuracy: 0.4350\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8487 - val_loss: 4.2081 - val_accuracy: 0.4400\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4132 - accuracy: 0.8438 - val_loss: 4.0267 - val_accuracy: 0.4500\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8650 - val_loss: 4.1308 - val_accuracy: 0.4300\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8350 - val_loss: 4.3480 - val_accuracy: 0.4450\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4323 - accuracy: 0.8438 - val_loss: 4.2088 - val_accuracy: 0.4500\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3851 - accuracy: 0.8413 - val_loss: 4.3158 - val_accuracy: 0.4450\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.8350 - val_loss: 3.8505 - val_accuracy: 0.4600\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8775 - val_loss: 4.1639 - val_accuracy: 0.4600\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.8512 - val_loss: 4.4338 - val_accuracy: 0.4300\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.8687 - val_loss: 4.4162 - val_accuracy: 0.4500\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8637 - val_loss: 4.2135 - val_accuracy: 0.4350\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3143 - accuracy: 0.8637 - val_loss: 4.4720 - val_accuracy: 0.4300\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8450 - val_loss: 4.5731 - val_accuracy: 0.4200\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8462 - val_loss: 4.7403 - val_accuracy: 0.4300\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8813 - val_loss: 4.7697 - val_accuracy: 0.4300\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4193 - accuracy: 0.8525 - val_loss: 4.3781 - val_accuracy: 0.4400\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8763 - val_loss: 4.8630 - val_accuracy: 0.4400\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8562 - val_loss: 4.7428 - val_accuracy: 0.4400\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.8537 - val_loss: 4.6736 - val_accuracy: 0.4450\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8550 - val_loss: 4.5323 - val_accuracy: 0.4400\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.8575 - val_loss: 5.1027 - val_accuracy: 0.4450\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8687 - val_loss: 4.6536 - val_accuracy: 0.4250\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8687 - val_loss: 4.6907 - val_accuracy: 0.4250\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8725 - val_loss: 4.8865 - val_accuracy: 0.4400\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8612 - val_loss: 4.8308 - val_accuracy: 0.4250\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8900 - val_loss: 4.7385 - val_accuracy: 0.4400\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8825 - val_loss: 4.8747 - val_accuracy: 0.4500\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8700 - val_loss: 5.0440 - val_accuracy: 0.4300\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8775 - val_loss: 4.9300 - val_accuracy: 0.4350\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8800 - val_loss: 4.9259 - val_accuracy: 0.4450\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8612 - val_loss: 4.8990 - val_accuracy: 0.4550\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8675 - val_loss: 4.6218 - val_accuracy: 0.4400\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8725 - val_loss: 4.7679 - val_accuracy: 0.4450\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8388 - val_loss: 4.8274 - val_accuracy: 0.4500\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8775 - val_loss: 4.9674 - val_accuracy: 0.4550\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8750 - val_loss: 4.9622 - val_accuracy: 0.4500\n",
      "\n",
      "\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.3364 - accuracy: 0.8840\n"
     ]
    }
   ],
   "source": [
    "neural_network_multi_class(\"label_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b363ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.9422 - accuracy: 0.2175 - val_loss: 1.9062 - val_accuracy: 0.3800\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.8744 - accuracy: 0.3562 - val_loss: 1.8609 - val_accuracy: 0.4000\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.7828 - accuracy: 0.4125 - val_loss: 1.8191 - val_accuracy: 0.4000\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.7083 - accuracy: 0.4313 - val_loss: 1.7718 - val_accuracy: 0.4000\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.6310 - accuracy: 0.4550 - val_loss: 1.7279 - val_accuracy: 0.4000\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.5354 - accuracy: 0.4938 - val_loss: 1.6868 - val_accuracy: 0.4050\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.4370 - accuracy: 0.5213 - val_loss: 1.7014 - val_accuracy: 0.4000\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.3420 - accuracy: 0.5263 - val_loss: 1.6622 - val_accuracy: 0.4200\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.3026 - accuracy: 0.5250 - val_loss: 1.6720 - val_accuracy: 0.4300\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.2270 - accuracy: 0.5612 - val_loss: 1.7302 - val_accuracy: 0.4200\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1921 - accuracy: 0.5738 - val_loss: 1.6757 - val_accuracy: 0.4350\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1725 - accuracy: 0.5925 - val_loss: 1.6197 - val_accuracy: 0.4500\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1456 - accuracy: 0.5838 - val_loss: 1.7278 - val_accuracy: 0.4350\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.1066 - accuracy: 0.6112 - val_loss: 1.7193 - val_accuracy: 0.4450\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0696 - accuracy: 0.6175 - val_loss: 1.7377 - val_accuracy: 0.4550\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0298 - accuracy: 0.6187 - val_loss: 1.7276 - val_accuracy: 0.4700\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9527 - accuracy: 0.6338 - val_loss: 1.7817 - val_accuracy: 0.4650\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9822 - accuracy: 0.6200 - val_loss: 1.9528 - val_accuracy: 0.4400\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9845 - accuracy: 0.6375 - val_loss: 1.8803 - val_accuracy: 0.4550\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9455 - accuracy: 0.6463 - val_loss: 1.8906 - val_accuracy: 0.4700\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9215 - accuracy: 0.6625 - val_loss: 1.8538 - val_accuracy: 0.4750\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9289 - accuracy: 0.6562 - val_loss: 1.8861 - val_accuracy: 0.4850\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.9161 - accuracy: 0.6550 - val_loss: 2.0034 - val_accuracy: 0.4400\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8969 - accuracy: 0.6787 - val_loss: 1.9142 - val_accuracy: 0.4350\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8719 - accuracy: 0.6950 - val_loss: 1.9958 - val_accuracy: 0.4750\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8451 - accuracy: 0.6837 - val_loss: 1.8681 - val_accuracy: 0.4650\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8629 - accuracy: 0.6963 - val_loss: 1.8273 - val_accuracy: 0.4950\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7661 - accuracy: 0.7362 - val_loss: 1.9259 - val_accuracy: 0.4850\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8149 - accuracy: 0.6950 - val_loss: 1.8452 - val_accuracy: 0.4750\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8356 - accuracy: 0.6850 - val_loss: 1.8891 - val_accuracy: 0.4800\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7523 - accuracy: 0.7312 - val_loss: 1.8978 - val_accuracy: 0.4950\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7868 - accuracy: 0.7237 - val_loss: 1.8025 - val_accuracy: 0.4850\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7503 - accuracy: 0.7250 - val_loss: 1.8234 - val_accuracy: 0.4950\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7786 - accuracy: 0.7237 - val_loss: 1.8668 - val_accuracy: 0.4700\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.8008 - accuracy: 0.7113 - val_loss: 1.7549 - val_accuracy: 0.4750\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7276 - accuracy: 0.7425 - val_loss: 1.8618 - val_accuracy: 0.4850\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7305 - accuracy: 0.7262 - val_loss: 1.9520 - val_accuracy: 0.4550\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7322 - accuracy: 0.7350 - val_loss: 1.9601 - val_accuracy: 0.4500\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6977 - accuracy: 0.7450 - val_loss: 1.9550 - val_accuracy: 0.4550\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6993 - accuracy: 0.7675 - val_loss: 2.0237 - val_accuracy: 0.4550\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.7650 - val_loss: 2.0208 - val_accuracy: 0.4600\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6772 - accuracy: 0.7538 - val_loss: 2.0380 - val_accuracy: 0.4550\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.7700 - val_loss: 1.9841 - val_accuracy: 0.4550\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.7613 - val_loss: 1.9667 - val_accuracy: 0.4600\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.7675 - val_loss: 2.0360 - val_accuracy: 0.4600\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6759 - accuracy: 0.7625 - val_loss: 1.9953 - val_accuracy: 0.4650\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.7005 - accuracy: 0.7700 - val_loss: 2.0348 - val_accuracy: 0.4600\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.7700 - val_loss: 1.9606 - val_accuracy: 0.4350\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6630 - accuracy: 0.7713 - val_loss: 1.9724 - val_accuracy: 0.4450\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.7525 - val_loss: 2.0731 - val_accuracy: 0.4600\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.7825 - val_loss: 2.1255 - val_accuracy: 0.4550\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6299 - accuracy: 0.7837 - val_loss: 2.0852 - val_accuracy: 0.4450\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.7875 - val_loss: 2.0679 - val_accuracy: 0.4400\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.7900 - val_loss: 2.1083 - val_accuracy: 0.4500\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.7763 - val_loss: 2.1400 - val_accuracy: 0.4500\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6459 - accuracy: 0.8075 - val_loss: 2.1629 - val_accuracy: 0.4650\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7950 - val_loss: 2.2128 - val_accuracy: 0.4350\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.8050 - val_loss: 2.1560 - val_accuracy: 0.4500\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.7912 - val_loss: 2.0972 - val_accuracy: 0.4350\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.7812 - val_loss: 2.1283 - val_accuracy: 0.4400\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6049 - accuracy: 0.7800 - val_loss: 2.1069 - val_accuracy: 0.4400\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.8000 - val_loss: 2.2027 - val_accuracy: 0.4600\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.7987 - val_loss: 2.1467 - val_accuracy: 0.4450\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.8213 - val_loss: 2.2227 - val_accuracy: 0.4450\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5819 - accuracy: 0.7975 - val_loss: 2.2555 - val_accuracy: 0.4450\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6247 - accuracy: 0.8037 - val_loss: 2.3120 - val_accuracy: 0.4500\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5751 - accuracy: 0.7950 - val_loss: 2.2623 - val_accuracy: 0.4350\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.8062 - val_loss: 2.1606 - val_accuracy: 0.4700\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.8050 - val_loss: 2.2049 - val_accuracy: 0.4450\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.8012 - val_loss: 2.2269 - val_accuracy: 0.4450\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.8125 - val_loss: 2.2456 - val_accuracy: 0.4400\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.8163 - val_loss: 2.3061 - val_accuracy: 0.4550\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.8112 - val_loss: 2.2971 - val_accuracy: 0.4450\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.7900 - val_loss: 2.2381 - val_accuracy: 0.4400\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.8012 - val_loss: 2.3132 - val_accuracy: 0.4400\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.8200 - val_loss: 2.3034 - val_accuracy: 0.4450\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.8087 - val_loss: 2.2429 - val_accuracy: 0.4700\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6114 - accuracy: 0.8062 - val_loss: 2.2576 - val_accuracy: 0.4450\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6092 - accuracy: 0.8037 - val_loss: 2.2713 - val_accuracy: 0.4350\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.8125 - val_loss: 2.2379 - val_accuracy: 0.4250\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.8062 - val_loss: 2.3557 - val_accuracy: 0.4450\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.8325 - val_loss: 2.5233 - val_accuracy: 0.4400\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5521 - accuracy: 0.8250 - val_loss: 2.4182 - val_accuracy: 0.4250\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.7937 - val_loss: 2.2898 - val_accuracy: 0.4250\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.8163 - val_loss: 2.3439 - val_accuracy: 0.4400\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.8125 - val_loss: 2.3857 - val_accuracy: 0.4350\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.8300 - val_loss: 2.4436 - val_accuracy: 0.4500\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.8250 - val_loss: 2.4851 - val_accuracy: 0.4450\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.8125 - val_loss: 2.4495 - val_accuracy: 0.4250\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.8375 - val_loss: 2.4802 - val_accuracy: 0.4350\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.8213 - val_loss: 2.4385 - val_accuracy: 0.4250\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.8462 - val_loss: 2.5361 - val_accuracy: 0.4300\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.8425 - val_loss: 2.4835 - val_accuracy: 0.4250\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.8487 - val_loss: 2.5452 - val_accuracy: 0.4500\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.8213 - val_loss: 2.5598 - val_accuracy: 0.4450\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.8350 - val_loss: 2.6032 - val_accuracy: 0.4500\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.8363 - val_loss: 2.6037 - val_accuracy: 0.4400\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.8275 - val_loss: 2.6180 - val_accuracy: 0.4500\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.8587 - val_loss: 2.6444 - val_accuracy: 0.4450\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8438 - val_loss: 2.6059 - val_accuracy: 0.4500\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.8350 - val_loss: 2.4402 - val_accuracy: 0.4400\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8350 - val_loss: 2.3899 - val_accuracy: 0.4400\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.8338 - val_loss: 2.3720 - val_accuracy: 0.4550\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.8550 - val_loss: 2.4913 - val_accuracy: 0.4550\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5422 - accuracy: 0.8213 - val_loss: 2.5256 - val_accuracy: 0.4500\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.8438 - val_loss: 2.5967 - val_accuracy: 0.4450\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.8062 - val_loss: 2.5366 - val_accuracy: 0.4200\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.8363 - val_loss: 2.5579 - val_accuracy: 0.4350\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.8338 - val_loss: 2.5839 - val_accuracy: 0.4350\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.8225 - val_loss: 2.6081 - val_accuracy: 0.4200\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.8288 - val_loss: 2.6788 - val_accuracy: 0.4300\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8525 - val_loss: 2.6994 - val_accuracy: 0.4600\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.8288 - val_loss: 2.6815 - val_accuracy: 0.4550\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.8300 - val_loss: 2.7008 - val_accuracy: 0.4300\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.8500 - val_loss: 2.5559 - val_accuracy: 0.4350\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.8338 - val_loss: 2.5538 - val_accuracy: 0.4400\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.8400 - val_loss: 2.4830 - val_accuracy: 0.4500\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.8388 - val_loss: 2.5427 - val_accuracy: 0.4450\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.8512 - val_loss: 2.5871 - val_accuracy: 0.4600\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.8575 - val_loss: 2.5874 - val_accuracy: 0.4400\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8562 - val_loss: 2.6546 - val_accuracy: 0.4350\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4505 - accuracy: 0.8550 - val_loss: 2.6902 - val_accuracy: 0.4400\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.8550 - val_loss: 2.6976 - val_accuracy: 0.4450\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.8275 - val_loss: 2.6811 - val_accuracy: 0.4350\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.8300 - val_loss: 2.7622 - val_accuracy: 0.4450\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4504 - accuracy: 0.8612 - val_loss: 2.7366 - val_accuracy: 0.4450\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.8550 - val_loss: 2.6860 - val_accuracy: 0.4500\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.8450 - val_loss: 2.7389 - val_accuracy: 0.4250\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8600 - val_loss: 2.7752 - val_accuracy: 0.4250\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.8562 - val_loss: 2.8214 - val_accuracy: 0.4200\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.8325 - val_loss: 2.8334 - val_accuracy: 0.4400\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.8562 - val_loss: 2.8593 - val_accuracy: 0.4400\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.8313 - val_loss: 2.8637 - val_accuracy: 0.4300\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8487 - val_loss: 2.8318 - val_accuracy: 0.4450\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.8487 - val_loss: 2.6709 - val_accuracy: 0.4600\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.8525 - val_loss: 2.7383 - val_accuracy: 0.4500\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.8487 - val_loss: 2.8194 - val_accuracy: 0.4350\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.8487 - val_loss: 2.7412 - val_accuracy: 0.4650\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.8462 - val_loss: 2.8157 - val_accuracy: 0.4450\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.8575 - val_loss: 2.8721 - val_accuracy: 0.4350\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4152 - accuracy: 0.8825 - val_loss: 2.7642 - val_accuracy: 0.4300\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.8450 - val_loss: 2.6403 - val_accuracy: 0.4350\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.8388 - val_loss: 2.6754 - val_accuracy: 0.4300\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8475 - val_loss: 2.7893 - val_accuracy: 0.4300\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.8300 - val_loss: 2.8965 - val_accuracy: 0.4250\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.8462 - val_loss: 2.8290 - val_accuracy: 0.4200\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8525 - val_loss: 2.8171 - val_accuracy: 0.4150\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.8462 - val_loss: 2.8009 - val_accuracy: 0.4300\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8612 - val_loss: 2.9036 - val_accuracy: 0.4250\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.8363 - val_loss: 2.7739 - val_accuracy: 0.4200\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.8338 - val_loss: 2.7884 - val_accuracy: 0.4450\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.8525 - val_loss: 2.6778 - val_accuracy: 0.4400\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.8537 - val_loss: 2.8486 - val_accuracy: 0.4350\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.8525 - val_loss: 2.9318 - val_accuracy: 0.4500\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8612 - val_loss: 2.8636 - val_accuracy: 0.4400\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.8562 - val_loss: 2.9766 - val_accuracy: 0.4200\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.8575 - val_loss: 3.0274 - val_accuracy: 0.4300\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8525 - val_loss: 3.0531 - val_accuracy: 0.4200\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.8550 - val_loss: 3.0925 - val_accuracy: 0.4250\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.8413 - val_loss: 2.8857 - val_accuracy: 0.4350\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.8338 - val_loss: 2.8325 - val_accuracy: 0.4200\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.8413 - val_loss: 2.7947 - val_accuracy: 0.4050\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.8388 - val_loss: 2.9173 - val_accuracy: 0.3900\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8712 - val_loss: 2.9649 - val_accuracy: 0.4100\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.8575 - val_loss: 2.8639 - val_accuracy: 0.4100\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.8575 - val_loss: 2.8385 - val_accuracy: 0.4050\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.8487 - val_loss: 2.9159 - val_accuracy: 0.4050\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.8575 - val_loss: 2.8667 - val_accuracy: 0.4250\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.8525 - val_loss: 2.8762 - val_accuracy: 0.4200\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8487 - val_loss: 2.8026 - val_accuracy: 0.3950\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.8537 - val_loss: 2.7342 - val_accuracy: 0.3950\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.8587 - val_loss: 2.7935 - val_accuracy: 0.4100\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.8637 - val_loss: 2.8240 - val_accuracy: 0.3850\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.8650 - val_loss: 2.8691 - val_accuracy: 0.4050\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.8612 - val_loss: 2.8592 - val_accuracy: 0.4100\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8750 - val_loss: 2.9503 - val_accuracy: 0.3950\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8625 - val_loss: 2.9972 - val_accuracy: 0.4050\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8800 - val_loss: 3.0173 - val_accuracy: 0.4200\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8562 - val_loss: 3.0165 - val_accuracy: 0.4050\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8587 - val_loss: 3.0348 - val_accuracy: 0.4350\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8662 - val_loss: 3.1238 - val_accuracy: 0.4100\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8625 - val_loss: 3.1663 - val_accuracy: 0.4150\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.8550 - val_loss: 3.2158 - val_accuracy: 0.4200\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8575 - val_loss: 3.1511 - val_accuracy: 0.4000\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8700 - val_loss: 3.2476 - val_accuracy: 0.4100\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8650 - val_loss: 3.2796 - val_accuracy: 0.4050\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8650 - val_loss: 3.2784 - val_accuracy: 0.4000\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8687 - val_loss: 3.3014 - val_accuracy: 0.4050\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8788 - val_loss: 3.3731 - val_accuracy: 0.4050\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8675 - val_loss: 3.4051 - val_accuracy: 0.4100\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8625 - val_loss: 3.2503 - val_accuracy: 0.4300\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.8525 - val_loss: 3.3054 - val_accuracy: 0.4150\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8350 - val_loss: 3.2797 - val_accuracy: 0.4000\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8737 - val_loss: 3.3971 - val_accuracy: 0.3950\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8675 - val_loss: 3.4641 - val_accuracy: 0.4000\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8662 - val_loss: 3.4482 - val_accuracy: 0.3850\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8600 - val_loss: 3.4071 - val_accuracy: 0.3900\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8575 - val_loss: 3.4058 - val_accuracy: 0.3900\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8637 - val_loss: 3.4627 - val_accuracy: 0.4000\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8712 - val_loss: 3.6164 - val_accuracy: 0.3900\n",
      "\n",
      "\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5990 - accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "neural_network_multi_class(\"label_lda_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c3e8c",
   "metadata": {},
   "source": [
    "**By using the above functions and neural network models, accuracy scores of multi-class and binary-class columns were checked.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
